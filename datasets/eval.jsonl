{"input": "What is the purpose of Hash tables in statistics?", "output": "When key-value pairs are stored in a hash table, the information regarding keys and associated values are stored in a hierarchical fashion using hash tables. The hashing function is used to provide an index that contains all of the information regarding keys and their associated values."}
{"input": "True or false: A perfect ensemble comprises of highly correct classifiers that differ as much as possible.", "output": "This is true."}
{"input": "How does selection sort work?", "output": "Selection sort works by picking the smallest number from the list and placing it at the front. This process is repeated for the second position towards the end of the list. It is the simplest sort algorithm."}
{"input": "What is the main key difference between supervised and unsupervised machine learning?", "output": "Supervised learning\tUnsupervised learning\n\nThe supervised learning technique needs labelled data to train the model. For example, to solve a classification problem (a supervised learning task), you need to have label data to train the model and to classify the data into your labelled groups.\tUnsupervised learning does not need any labelled dataset. This is the main key difference between supervised learning and unsupervised learning.\n"}
{"input": "What is the relationship between standard error and margin of error?", "output": "Margin of error = Critical value X Standard deviation for the population and Margin of error = Critical value X Standard error of the sample. The margin of error will increase with the standard error."}
{"input": "There are many machine learning algorithms till now. If given a data set, how can one determine which algorithm to be used for that?", "output": "Machine Learning algorithm to be used purely depends on the type of data in a given dataset. If data is linear then, we use linear regression. If data shows non-linearity then, the bagging algorithm would do better. If the data is to be analyzed/interpreted for some business purposes then we can use decision trees or SVM. If the dataset consists of images, videos, audios then, neural networks would be helpful to get the solution accurately.\n\nSo, there is no certain metric to decide which algorithm to be used for a given situation or a data set. We need to explore the data using EDA (Exploratory Data Analysis) and understand the purpose of using the dataset to come up with the best fit algorithm. So, it is important to study all the algorithms in detail.\n\nMachine Learning with Python | Machine Learning Tutorial for Beginners | Machine Learning Tutorial\n"}
{"input": "Complete the sentence: The relative entropy D(p||q) is the measure of (a) [___] between two distributions. It can also be expressed as a measure of the (b)[___] of assuming that the distribution is q when the (c)[___] distribution is p.", "output": "The relative entropy D(p||q) is the measure of difference between two distributions. It can also be expressed like a measure of the inefficiency of assuming that the distribution is q when the true distribution is p."}
{"input": " Explain the access modifiers?", "output": "Access modifiers are a way of specifying which users have access to a resource. Access modifiers are often used to control access to data, files, or other resources. For example, a database might allow only certain users to see the data. Or, a file might be locked down so that only the owner can see it."}
{"input": "What is the purpose of differencing a time series dataset?", "output": "Differencing a time series dataset is used to make the data stationary, which means making the mean and variance constant over time. This is crucial because many statistical models require the data to be stationary for accurate predictions. Differencing achieves this by subtracting the previous observation from the current one, thereby eliminating trends and seasonality in the data. It\u2019s an essential step in time series forecasting as it improves model performance by reducing bias due to non-stationarity."}
{"input": " What is SDLC?", "output": "SDLC stands for Software Development Life Cycle is a process that produces quality software products in less time. The stages involve by SDLC are: 1) planning, 2) design, 4) construction, 5) testing, and 6) deployment."}
{"input": "What would be some issues if blogs decide to cover one of your experimental groups?", "output": "Same as the previous question. The above problem can happen in larger scale."}
{"input": "What are the differences between parametric and non-parametric statistical models?", "output": "Parametric models assume underlying patterns in the data, typically involving parameters or distributions. They require less data for accurate predictions but can be misleading if assumptions are incorrect. Non-parametric models make fewer assumptions about data distribution and are more flexible, allowing them to handle diverse datasets better. However, they often need larger sample sizes to achieve similar accuracy levels as parametric models."}
{"input": "Explain Probability Sampling and provide a few examples?", "output": "The sampling technique which allows each and every data value from the population to be selected into the sample with equal opportunity is known as Probability sampling or Unbiased sampling. Examples of Unbiased Sampling are: Simple Random Sampling, Systematic Sampling, Stratified Sampling, Clustered Sampling"}
{"input": "What is the meaning of sensitivity in statistics?", "output": "Sensitivity refers to the accuracy of a classifier in a test. It can be calculated using the formula \u2013 Sensitivity = Predicted True Events/Total number of Events"}
{"input": "In NLP, Words represented as vectors are called Neural Word Embeddings a. True b. False", "output": "Answer: a)\n\nWord2Vec, GloVe based models build word embedding vectors that are multidimensional.\n"}
{"input": " What is inheritance?", "output": "Inheritance is a way of structuring code so that it can be shared between different classes. Inheritance allows code to be shared between different classes and allows the same class to have different implementations in different contexts. Inheritance is a powerful tool that can make your code more maintainable and easier to understand. Inheritance is a fundamental concept in programming. It\u2019s a way of structuring code so that it can be shared between different classes."}
{"input": "When is it better to use an F1 score over accuracy to evaluate a model?", "output": "The F1 score is the harmonic mean of a model\u2019s precision and recall. In the case of imbalanced datasets, the F1 score provides us a better measure of model performance than accuracy."}
{"input": "What are some methods to check if a time series is stationary or not?", "output": "Stationarity in a time series implies that its statistical properties do not change over time. To check for stationarity, we can use visual methods and statistical tests. Visual inspection involves plotting the data to observe if there\u2019s any obvious trend or seasonality. The mean, variance, and covariance should remain constant over time for a stationary series. Autocorrelation function (ACF) plot is another visual tool. If the series is stationary, autocorrelations will drop to zero relatively quickly. Statistical tests like Augmented Dickey-Fuller (ADF), Kwiatkowski-Phillips-Schmidt-Shin (KPSS), and Phillips-Perron (PP) are commonly used. ADF null hypothesis assumes presence of unit root i.e., non-stationary series. Rejecting it indicates stationarity. KPSS test does the opposite; its null hypothesis assumes stationarity. PP test improves upon ADF by considering heteroscedasticity. Remember, these methods have limitations and results should be interpreted with caution."}
{"input": "  What is netstat?", "output": "Netstat is a command-line utility program. It provides useful information about the current TCP/IP settings of a connection."}
{"input": "How can we use a dataset without the target variable into supervised learning algorithms?", "output": "Input the data set into a clustering algorithm, generate optimal clusters, label the cluster numbers as the new target variable. Now, the dataset has independent and target variables present. This ensures that the dataset is ready to be used in supervised learning algorithms. \n"}
{"input": "What is the meaning of covariance?", "output": "In statistics, covariance is a measure of association between two random variables from their respective means in a cycle. "}
{"input": "Define the term \u201cheteroscedasticity\u201d and explain how it can impact model performance.", "output": "Heteroscedasticity refers to a situation in statistical modeling where the variability of the error term, or residual variance, is not constant across all levels of the independent variables. This violates one of the key assumptions of ordinary least squares (OLS) regression, which assumes homoscedasticity, or equal variance. When heteroscedasticity is present, it can lead to inefficient parameter estimates because OLS gives more weight to observations with larger variances. Consequently, standard errors may be biased leading to unreliable hypothesis tests and confidence intervals. Moreover, model performance metrics like R-squared could be inflated, giving an overly optimistic view of the model\u2019s predictive power. To mitigate these issues, various techniques such as weighted least squares (WLS), robust standard errors, or transforming the dependent variable can be used. These methods aim to correct for unequal variances and provide more reliable parameter estimates and inference."}
{"input": "How deep learning is used in supervised, unsupervised as well as reinforcement machine learning?", "output": "Deep learning can be used for supervised, unsupervised as well as reinforcement machine learning. it uses a variety of ways to process these. \n Supervised Machine Learning: Supervised machine learning is the machine learning technique in which the neural network learns to make predictions or classify data based on the labeled datasets. Here we input both input features along with the target variables. the neural network learns to make predictions based on the cost or error that comes from the difference between the predicted and the actual target, this process is known as backpropagation.  Deep learning algorithms like Convolutional neural networks, Recurrent neural networks are used for many supervised tasks like image classifications and recognization, sentiment analysis, language translations, etc. \n Unsupervised Machine Learning: Unsupervised machine learning is the machine learning technique in which the neural network learns to discover the patterns or to cluster the dataset based on unlabeled datasets. Here there are no target variables. while the machine has to self-determined the hidden patterns or relationships within the datasets. Deep learning algorithms like autoencoders and generative models are used for unsupervised tasks like clustering, dimensionality reduction, and anomaly detection. \n Reinforcement  Machine Learning: Reinforcement  Machine Learning is the machine learning technique in which an agent learns to make decisions in an environment to maximize a reward signal. The agent interacts with the environment by taking action and observing the resulting rewards. Deep learning can be used to learn policies, or a set of actions, that maximizes the cumulative reward over time. Deep reinforcement learning algorithms like Deep Q networks and Deep Deterministic Policy Gradient (DDPG) are used to reinforce tasks like robotics and game playing etc."}
{"input": "What are preprocessor directives?", "output": "Preprocessor directives are placed at the beginning of every C program. This is where library files are specified, which would depend on what functions are to be used in the program. Another use of preprocessor directives is the declaration of constants.Preprocessor directives begin with the # symbol."}
{"input": "What are some benefits of Naive Bayes?", "output": "It works better than simple algorithms like logistic regression etc. It also works well with categorical data and with numerical data as well. Additionally, It is very easy and fast to work with the Naive Bayes classifier. Complex and high dimensional data is well suited for Naive Bayes classifier. It can also be trained using a small labeled dataset with semi-supervised learning. In other words: It performs well with both clean and noisy data. Training takes a few samples, but the fundamental assumption is that the training dataset is a genuine representation of the population. Obtaining the likelihood of a forecast is simple."}
{"input": "hat Is TCP Three-Way Handshake?", "output": "THREE-WAY handshake or a TCP 3-way handshake is a process that is used in a TCP/IP network to make a connection between the server and client. It is a three-step process that requires both the client and server to exchange synchronization and acknowledgment packets before the real data communication process starts."}
{"input": "When can be a categorical value treated as a continuous variable and what effect does it have when done so?", "output": "Ans. A categorical predictor can be treated as a continuous one when the nature of data points it represents is ordinal. If the predictor variable is having ordinal data then it can be treated as continuous and its inclusion in the model increases the performance of the model.\n"}
{"input": "What are some real-world use cases for Bayesian statistics?", "output": "Bayesian statistics can be used for a variety of purposes, including medical diagnosis, weather forecasting, and spam filtering."}
{"input": "What is an unbiased estimator? Give an example for a layperson.", "output": "An unbiased estimator is an accurate statistic that is used to approximate a population parameter. An example would be taking a sample of 10,000 voters in a political poll to estimate the total voting population. There is no such thing as a perfectly unbiased estimator because this would require you to accurately survey the entire population for your sample, impossible amongst what is often millions of eligible respondents."}
{"input": "What are hyperparameters and how are they different from parameters?", "output": "A parameter is a variable that is internal to the model and whose value is estimated from the training data. They are often saved as part of the learned model. Examples include weights, biases etc.\n\nA hyperparameter is a variable that is external to the model whose value cannot be estimated from the data. They are often used to estimate model parameters. The choice of parameters is sensitive to implementation. Examples include learning rate, hidden layers etc.\n"}
{"input": "What is the use of LSTM?", "output": "LSTM stands for long short-term memory. It is a type of RNN that is used to sequence a string of data. It consists of feedback chains that give it the ability to perform like a general-purpose computational entity."}
{"input": " Compare and contrast compilers from interpreters.", "output": "Compilers and interpreters often deal with how program codes are executed. Interpreters execute program codes one line at a time, while compilers take the program as a whole and convert it into object code, before executing it. The key difference here is that in the case of interpreters, a program may encounter syntax errors in the middle of execution, and will stop from there. On the other hand, compilers check the syntax of the entire program and will only proceed to execution when no syntax errors are found."}
{"input": "What is the difference between Entropy and Information Gain?", "output": "The information gain is based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding the attribute that returns the highest information gain (i.e., the most homogeneous branches). Step 1: Calculate entropy of the target.\n"}
{"input": "How can you detect outliers?", "output": "Detecting outliers is as simple as defining the difference. Outliers are nothing but observations that can differ differently from other observations and the easiest way you can plot the variable is by detecting the data points which are far from others. Now the only way to quantify such differences is by using quartiles or interquartile range (IQR). Interquartile Range can be detected when you minus the first quartile i.e. Q3-Q1. The outliers can be defined as any data point which is lesser than Q1\u20131.5*IQR or maybe higher than Q3+1.5*IQR."}
{"input": "What is time- sharing system?", "output": "In a Time-sharing system, the CPU executes multiple jobs by switching among them, also known as multitasking. This process happens so fast that users can interact with each program while it is running."}
{"input": "What are the different ways of securing a computer network?", "output": "There are several ways to do this. Install a reliable and updated anti-virus program on all computers. Make sure firewalls are setup and configured correctly. User authentication will also help a lot. All these combined would make a highly secured network."}
{"input": "What is the proportion of confidence intervals that will not contain the population parameter?", "output": "Alpha is the probability in a confidence interval that will not contain the population parameter. \u03b1 = 1 \u2013 CL. Alpha is usually expressed as a proportion. For instance, if the confidence level is 95%, then alpha would be equal to 1-0.95 or 0.05."}
{"input": " How are server systems classified?", "output": "Server systems can be classified as either computer-server systems or file server systems. In the first case, an interface is made available for clients to send requests to perform an action. In the second case, provisions are available for clients to create, access and update files."}
{"input": "In NLP, Tokens are converted into numbers before giving to any Neural Network a. True b. False", "output": "Answer: a)\n\nIn NLP, all words are converted into a number before feeding to a Neural Network.\n"}
{"input": "  What is ANN?", "output": " Artificial Neural Network (ANN) is a computational model based on the structure of the Biological Neural Network(BNN). The human brain has billions of neurons that collect, process the information, and drive meaningful results out of it. The neurons use electro-chemical signals to communicate and pass the information to other neurons. Similarly, ANN consists of artificial neurons called nodes connected with other nodes forming a complex relationship between the output and the input. "}
{"input": "What are overfitting and underfitting? Why does the decision tree algorithm suffer often with overfitting problems?", "output": "Overfitting is a statistical model or machine learning algorithm that captures the data\u2019s noise. Underfitting is a model or machine learning algorithm which does not fit the data well enough and occurs if the model or algorithm shows low variance but high bias.\n\nIn decision trees, overfitting occurs when the tree is designed to fit all samples in the training data set perfectly. This results in branches with strict rules or sparse data and affects the accuracy when predicting samples that aren\u2019t part of the training set.\n\nAlso Read: Overfitting and Underfitting in Machine Learning\n"}
{"input": "How do you identify outliers?", "output": "An outlier is an observation that is unlikely to have occurred in a data set under ordinary circumstances. Its values are so widely different from the other observations that it is most likely a result of noise or a rare exceptional case. Box plots are a simple, effective, and hence commonly used approach to identifying outliers. Besides this, scatter plots, histograms and Z-scores are other methods used whenever feasible."}
{"input": "Write Shannon's famous general formulae for uncertainty", "output": "Shannons famous general formulae for uncertainty is: H = -sum^{N}_{a=1} P_a log_2 P_a(bits per symbol)."}
{"input": "How do you interpret the results of an ANOVA test?", "output": "The ANOVA test produces an F-statistic and a p-value. The F-statistic measures the ratio of the variation between group means to the variation within groups. If the F-statistic is significant and the p-value is below the chosen significance level (e.g., 0.05), we reject the null hypothesis and conclude that at least one group mean significantly differs from the others. Post-hoc tests are then conducted to identify which specific group means differ significantly."}
{"input": "How can you incorporate external factors or events into a time series model?", "output": "Incorporating external factors into a time series model involves the use of exogenous variables. These are variables that aren\u2019t part of the time series data but have an impact on it. For instance, in predicting sales, holidays or promotional events could be exogenous variables. To incorporate these, you can create binary dummy variables representing the occurrence of these events. If the event occurs at a particular time point, the variable is 1; otherwise, it\u2019s 0. This way, the model captures the effect of these events on the target variable. Another approach is to use intervention analysis if the event is not regular but has a significant impact. Here, you modify the time series data by adding a term representing the event\u2019s impact. Lastly, for continuous external factors like temperature in weather forecasting, you can directly include them as additional input features to your model."}
{"input": "What is a Generative Adversarial Network (GAN)?", "output": "Generative Adversarial Networks (GANs) are a type of neural network architecture used for unsupervised learning tasks like image synthesis and generative modeling. It is composed of two neural networks: Generator and Discriminator. The generator takes the random distributions mainly the Gaussian distribution as inputs and generates the synthetic data, while the discriminator takes both real and synthetic data as input and predicts whether the input is real or synthetic. The goal of the generator is to generate synthetic data that is identical to the input data. and the discriminator guesses whether the input data is real or synthetic."}
{"input": "What do you mean by destructor?", "output": "Destructor is a class member function that deletes or destructs an object."}
{"input": "What do you mean by the ROC curve?", "output": "Receiver operating characteristics (ROC curve): ROC curve illustrates the diagnostic ability of a binary classifier. It is calculated/created by plotting True Positive against False Positive at various threshold settings. The performance metric of ROC curve is AUC (area under curve). Higher the area under the curve, better the prediction power of the model.\n"}
{"input": "What is SMTP?", "output": "SMTP is short for Simple Mail Transfer Protocol. This protocol deals with all internal mail and provides the necessary mail delivery services on the TCP/IP protocol stack."}
{"input": "What are deep autoencoders?", "output": "Two symmetrical deep-belief networks which typically have four or five shallow layers that represent the encoding half of the net and a second set of four or five layers that represent the decoding half are together known as deep autoencoder. These layers are restricted Boltzmann machines and the building blocks of deep belief networks.\n\nTo process the dataset MNIST, a deep autoencoder uses binary transformations after each RBM. They can also be used for other datasets on which you would use Gaussian rectified transformations instead of the RBM.  \n"}
{"input": " What is Documentation?", "output": "Documentation is a detailed description of a program\u2019s algorithm, design, coding method, testing, and proper usage. Documentation is valuable for the users who rely upon the program on a day-to-day basis, and for the programmer who may be called on to modify or update it."}
{"input": "What is a private IP address?", "output": "Private IP addresses are assigned for use on intranets. These addresses are used for internal networks and are not routable on external public networks. These ensure that no conflicts are present among internal networks. At the same time, the same range of private IP addresses is reusable for multiple intranets since they do not \u201csee\u201d each other."}
{"input": "What is a bootstrap program in OS?", "output": "It is generally a program that initializes OS during startup i.e., first code that is executed whenever computer system startups. OS is loaded through a bootstrapping process or program commonly known as booting. Overall OS only depends on the bootstrap program to perform and work correctly. It is fully stored in boot blocks at a fixed location on the disk. It also locates the kernel and loads it into the main memory after which the program starts its execution."}
{"input": "Can you discuss how the bootstrap method is used in resampling techniques?", "output": "The bootstrap method is a resampling technique used in statistics to estimate the sampling distribution of a statistic. It involves generating multiple samples from an original dataset, each with replacement, and then calculating the desired statistic for each sample. The resulting distribution of these calculated statistics provides an approximation of the true sampling distribution. This approach allows us to make statistical inferences when traditional methods are not applicable or feasible due to complex data structures or small sample sizes."}
{"input": " What is a Hypothesis in Machine Learning?", "output": "A hypothesis is a term that is generally used in the Supervised machine learning domain. As we have independent features and target variables and we try to find an approximate function mapping from the feature space to the target variable that approximation of mapping is known as a hypothesis. "}
{"input": "True or False: Using Jensen's inequality, it is possible to show that the KL divergence is always greater or equal to zero.", "output": "True The non-negativity of KLD can be proved using Jensen's inequality."}
{"input": "Why is the operating system important?", "output": "OS is the most essential and vital part of a computer without which it is considered useless. It enables an interface or acts like a link for interaction between computer software that is installed on OS and users. It also helps to communicate with hardware and also maintains balance among hardware and CPU. It also provides services to users and a platform for programs to run on. It performs all common tasks applications require.  "}
{"input": "Which distance do we measure in the case of KNN?", "output": "Ans. The hamming distance is measured in case of KNN for the determination of nearest neighbours. Kmeans uses euclidean distance.\n"}
{"input": " What is the backbone network?", "output": "There are 7 OSI layers: 1) Physical Layer, 2) Data Link Layer, 3) Network Layer, 4) Transport Layer, 5) Session Layer, 6) Presentation Layer, and 7) Application Layer."}
{"input": "What are the advantages and disadvantages of using Markov Chain Monte Carlo methods in Bayesian Inference?", "output": "Markov Chain Monte Carlo (MCMC) methods are powerful tools in Bayesian inference, offering several advantages. They allow for the estimation of complex models where analytical solutions may not be feasible. MCMC can handle high-dimensional parameter spaces and provide estimates for any function of parameters. However, they also have disadvantages. The convergence to a stationary distribution can be slow, especially in high dimensions or when the target distribution is multimodal. This makes it computationally expensive. Additionally, determining whether convergence has been achieved can be challenging."}
{"input": "What is Deep Learning?", "output": "Deep learning is the branch of machine learning which is based on artificial neural network architecture which makes it capable of learning complex patterns and relationships within data. An artificial neural network or ANN uses layers of interconnected nodes called neurons that work togeather to process and learn from the input data. In a fully connected Deep neural network, there is an input layer and one or more hidden layers connected one after the other. Each neuron receives input from the previous layer neurons or the input layer. The output of one neuron becomes the input to other neurons in the next layer of the network, and this process continues until the final layer produces the output of the network. The layers of the neural network transform the input data through a series of nonlinear transformations, allowing the network to learn complex representations of the input data. Today Deep learning has become one of the most popular and visible areas of machine learning, due to its success in a variety of applications, such as computer vision, natural language processing, and Reinforcement learning."}
{"input": "How do you implement multinomial logistic regression?", "output": "The multinomial logistic classifier can be implemented using a generalization of the sigmoid, called the softmax function. The softmax represents each class with a value in the range (0,1), with all the values summing to 1. Alternatively, you could use the one-vs-all or one-vs-one approach using multiple simple binary classifiers."}
{"input": "What is the disadvantage of a star topology?", "output": "One major disadvantage of star topology is that once the central Hub or switch gets damaged, the entire network becomes unusable."}
{"input": "What is the definition of selection bias?", "output": "The process of selecting individual or group data in a way that is not random is known as selection bias. Randomization is crucial in evaluating model functionality and performing analysis. Therefore, if incorrect randomization is not avoided, the obtained sample will not accurately represent the population."}
{"input": "What is Multilayer Perceptron and Boltzmann Machine?", "output": "Similar to single layer perceptron, multilayer perceptrons have input, output and hidden layers. However, since MLPs have more than one hidden layer, they are capable of classifying non-linear classes. Each node in the hidden layers uses a nonlinear activation function along with the input layers to produce the output through \u2018backpropagation\u2019. In this method, the neural networks calculate the errors using cost function and pushes the error backwards to the source to train the model more accurately.\n\nThe Boltzmann machine is a simplified version of the multilayer perceptron. This is a two layer model with a visible input layer and a hidden layer which makes stochastic decisions for the neurons. The nodes of this model are connected across layers without being connected to each other.\n"}
{"input": "  How many layers are in a Neural Network?", "output": "Neural networks are one of many types of ML algorithms that are used to model complex patterns in data. They are composed of three layers \u2014 input layer, hidden layer, and output layer."}
{"input": "Between SVM and logistic regression, which algorithm is most likely to work better in the presence of outliers? Why?", "output": "SVM is capable of handling outliers better than logistic regression. SVM is affected only by the points closest to the decision boundary. Logistic regression, on the other hand, tries to maximize the conditional likelihood of the training data and is therefore strongly affected by the presence of outliers."}
{"input": "What is POP3?", "output": "POP3 stands for Post Office Protocol version3. POP is responsible for accessing the mail service on a client machine. POP3 works on two models such as Delete mode and Keep mode."}
{"input": " What is Machine code?", "output": "Machine code is a language, which can be processed directly by a microprocessor without any need of the previous transformation. Programmers never write programs directly in machine code."}
{"input": "Briefly explain recursive algorithm.", "output": "Recursive algorithm targets a problem by dividing it into smaller, manageable sub-problems. The output of one recursion after processing one sub-problem becomes the input to the next recursive process."}
{"input": "What are the advantages and disadvantages of using an Array?", "output": "Advantages:\n\nRandom access is enabled\n\nSaves memory\n\nCache friendly\n\nPredictable compile timing\n\nHelps in re-usability of code\n\nDisadvantages: \n\nAddition and deletion of records is time consuming even though we get the element of interest immediately through random access. This is due to the fact that the elements need to be reordered after insertion or deletion.\n\nIf contiguous blocks of memory are not available in the memory, then there is an overhead on the CPU to search for the most optimal contiguous location available for the requirement.\n\nNow that we know what arrays are, we shall understand them in detail by solving some interview questions. Before that, let us see the functions that Python as a language provides for arrays, also known as, lists.\n\nappend() \u2013 Adds an element at the end of the list\n\ncopy() \u2013 returns a copy of a list.\n\nreverse() \u2013 reverses the elements of the list\n\nsort() \u2013 sorts the elements in ascending order by default.\n"}
{"input": "What does the term Variance Inflation Factor mean?", "output": "Variation Inflation Factor (VIF) is the ratio of the model\u2019s variance to the model\u2019s variance with only one independent variable. VIF gives the estimate of the volume of multicollinearity in a set of many regression variables.\n\nVIF = Variance of the model with one independent variable\n"}
{"input": "Can you define confidence level?", "output": "The confidence level defined in the hypothesis testing is said to be the probability of rejecting a null hypothesis provided it is a false one. The formula to calculate this is, P(Not Rejecting H0|H0 is True) = 1 \u2014 P(Rejecting H0|H0 is True) Where the default statistical power is said to be at 95 percent."}
{"input": "Why should you care about p-values when dealing with Bayesian models?", "output": "P-values are important when dealing with Bayesian models because they can help you to determine whether or not your model is a good fit for the data. If the p-value is too high, then it means that your model is not a good fit for the data and you should look for a different model."}
{"input": ". Define ensemble learning in the context of machine learning.", "output": ". Ensemble learning is an excellent machine learning idea which displays noticeable benefits in many applications, one such notable example is the widespread use of ensembles in Kaggle competitions. In an ensemble several individual models (for instance ResNet18 and VGG16) which were trained on the same corpus, work in tandem and during inference, their predictions are fused by a pre-defined strategy to yield a single prediction."}
{"input": "List out Layers of OSI Model", "output": "DLayers of OSI models are: 1) Physical, 2) Application, 3) Presentation, 4) Session, 5) Transport, 6) Network, and 6) Data Link Layer."}
{"input": "How does a selection sort work for an array?", "output": "The selection sort is a fairly intuitive sorting algorithm, though not necessarily efficient. In this process, the smallest element is first located and switched with the element at subscript zero, thereby placing the smallest element in the first position.The smallest element remaining in the subarray is then located next to subscripts 1 through n-1 and switched with the element at subscript 1, thereby placing the second smallest element in the second position. The steps are repeated in the same manner till the last element."}
{"input": "Differentiate STACK from ARRAY.", "output": "Stack follows a LIFO pattern. It means that data access follows a sequence wherein the last data to be stored when the first one to be extracted. Arrays, on the other hand, does not follow a particular order and instead can be accessed by referring to the indexed element within the array."}
{"input": "What is the difference between Machine Learning and Deep Learning?", "output": "Machine Learning forms a subset of Artificial Intelligence, where we use statistics and algorithms to train machines with data, thereby, helping them improve with experience. Deep Learning is a part of Machine Learning, which involves mimicking the human brain in terms of structures called neurons, thereby, forming neural networks."}
{"input": "What do you mean by Corpus in NLP?", "output": "In NLP, a corpus is a huge collection of texts or documents. It is a structured dataset that acts as a sample of a specific language, domain, or issue. A corpus can include a variety of texts, including books, essays, web pages, and social media posts. Corpora are frequently developed and curated for specific research or NLP objectives. They serve as a foundation for developing language models, undertaking linguistic analysis, and gaining insights into language usage and patterns."}
{"input": "How can you use the chi-square test in mathematical statistics? Provide examples.", "output": "The chi-square test is a statistical method used to determine if there\u2019s a significant association between two categorical variables in a sample. It compares observed frequencies with expected frequencies based on certain assumptions. For example, consider an experiment where we toss a coin 100 times and observe the outcomes. If it\u2019s a fair coin, we expect heads and tails to occur equally often \u2013 50 times each. However, suppose we get 60 heads and 40 tails. We can use the chi-square test to assess whether this deviation from expectation is due to chance or indicates that the coin might be biased. In this case, our null hypothesis (H0) would be \u201cthe coin is fair\u201d, and the alternative hypothesis (Ha) would be \u201cthe coin is not fair\u201d. The observed frequencies are 60 (heads) and 40 (tails), while the expected frequencies under H0 are 50 each. We calculate the chi-square statistic as follows: \u03c7\u00b2 = \u03a3 [ (O-E)\u00b2 / E ]. where O represents observed frequency and E represents expected frequency for each category. In our case, \u03c7\u00b2 = (60-50)\u00b2/50 + (40-50)\u00b2/50 = 4. Then, we compare this value with a critical value from the chi-square distribution table. If \u03c7\u00b2 exceeds the critical value, we reject H0 and conclude that the coin may be biased."}
{"input": "What is the left-skewed distribution and the right-skewed distribution?", "output": "In the left-skewed distribution, the left tail is longer than the right side. Mean < median < mode. In the right-skewed distribution, the right tail is longer. It is also known as positive-skew distribution. Mode < median < mean"}
{"input": "True or False: SGD usually avoids the trap of poor local minima.", "output": "True."}
{"input": "When your read that someone penalized the L2 norm, was the euclidean or the Manhattan distance involved?", "output": "The Manhattan distance is an L1 norm (introduced by Hermann Minkowski) while the Euclidean distance is an L2 norm."}
{"input": "  What are the advantages and disadvantages of a heap?", "output": "Storing data on the heap is slower than it would take when using the stack. However, the main advantage of using the heap is its flexibility. That\u2019s because memory in this structure can be allocated and remove in any particular order. Slowness in the heap can be compensated if an algorithm was well designed and implemented."}
{"input": "What is Latent Semantic Indexing (LSI)?", "output": "Latent semantic indexing is a mathematical technique used to improve the accuracy of the information retrieval process. The design of LSI algorithms allows machines to detect the hidden (latent) correlation between semantics (words). To enhance information understanding, machines generate various concepts that associate with the words of a sentence. The technique used for information understanding is called singular value decomposition. It is generally used to handle static and unstructured data. The matrix obtained for singular value decomposition contains rows for words and columns for documents. This method is best suited to identify components and group them according to their types. The main principle behind LSI is that words carry a similar meaning when used in a similar context. Computational LSI models are slow in comparison to other models. However, they are good at contextual awareness which helps improve the analysis and understanding of a text or a document."}
{"input": "What is probability, and how is it used in data science?", "output": "Probability is a measure of the likelihood of an event occurring. Data science, helps us make informed decisions in uncertain situations. For instance, in predicting customer churn, I can use probability to estimate the likelihood of a customer leaving based on historical data. This enables businesses to prioritize retention efforts and allocate resources more effectively."}
{"input": "If the deviations of the residuals from your model are extremely small, does it suggest that the model is good or bad?", "output": "Residuals are essentially how much the actual data points vary from the fitted line and are hence indicators of deviation or error. Therefore, the smaller the deviation of the residuals from the fitted line, the better the model is likely to be."}
{"input": " What is an Assembler?", "output": "An assembler acts as a translator for low-level language. Assembly codes written using mnemonic commands are translated by the Assembler into machine language."}
{"input": " What is artificial intelligence?", "output": "Artificial Intelligence or machine intelligence is a common term that is used to build smart machines capable of performing tasks. The main aim of AI is to solve problems in a way that are better and faster."}
{"input": "What are the steps to be followed to use the gradient descent algorithm?", "output": "There are five main steps that are used to initialize and use the gradient descent algorithm: Initialize biases and weights for the network, Send input data through the network (the input layer), Calculate the difference (the error) between expected and predicted values, Change values in neurons to minimize the loss function, Multiple iterations to determine the best weights for efficient working"}
{"input": "How does one define statistical interaction?", "output": "When an input variable influences an output variable, a statistical interaction occurs. In real life, for example, the interaction of adding sugar to the stirring of tea is an example of statistical interaction. Neither of the variables has an impact on sweetness, but the two variables combine to produce sweetness."}
{"input": "Explain the phrase \u201cCurse of Dimensionality\u201d.", "output": "The Curse of Dimensionality refers to the situation when your data has too many features.\n\nThe phrase is used to express the difficulty of using brute force or grid search to optimize a function with too many inputs.\n\nIt can also refer to several other issues like:\n\nIf we have more features than observations, we have a risk of overfitting the model.\n\nWhen we have too many features, observations become harder to cluster. Too many dimensions cause every observation in the dataset to appear equidistant from all others and no meaningful clusters can be formed.\n\nDimensionality reduction techniques like PCA come to the rescue in such cases.\n"}
{"input": "How to detect outliers?", "output": "The best way to detect outliers is through graphical means. Apart from that, outliers can also be detected through the use of statistical methods using tools such as Excel, Python, SAS, among others. The most popular graphical ways to detect outliers include box plot and scatter plot."}
{"input": "List the most popular distribution curves along with scenarios where you will use them in an algorithm.", "output": "The most popular distribution curves are as follows- Bernoulli Distribution, Uniform Distribution, Binomial Distribution, Normal Distribution, Poisson Distribution, and Exponential Distribution. Check out the free Probability for Machine Learning course to enhance your knowledge on Probability Distributions for Machine Learning.\n\nEach of these distribution curves is used in various scenarios.\n\nBernoulli Distribution can be used to check if a team will win a championship or not, a newborn child is either male or female, you either pass an exam or not, etc.\n\nUniform distribution is a probability distribution that has a constant probability. Rolling a single dice is one example because it has a fixed number of outcomes.\n\nBinomial distribution is a probability with only two possible outcomes, the prefix \u2018bi\u2019 means two or twice. An example of this would be a coin toss. The outcome will either be heads or tails.\n\nNormal distribution describes how the values of a variable are distributed. It is typically a symmetric distribution where most of the observations cluster around the central peak. The values further away from the mean taper off equally in both directions. An example would be the height of students in a classroom.\n\nPoisson distribution helps predict the probability of certain events happening when you know how often that event has occurred. It can be used by businessmen to make forecasts about the number of customers on certain days and allows them to adjust supply according to the demand.\n\nExponential distribution is concerned with the amount of time until a specific event occurs. For example, how long a car battery would last, in months.\n"}
{"input": "What are some of the examples of supervised learning algorithms in Deep Learning?", "output": "There are three main supervised learning algorithms in Deep Learning: Artificial neural networks, Convolutional neural networks, Recurrent neural networks"}
{"input": "What is Naive Bayes algorithm, When we can use this algorithm in NLP?", "output": "Naive Bayes algorithm is a collection of classifiers which works on the principles of the Bayes\u2019 theorem. This series of NLP model forms a family of algorithms that can be used for a wide range of classification tasks including sentiment prediction, filtering of spam, classifying documents and more.\n\nNaive Bayes algorithm converges faster and requires less training data. Compared to other discriminative models like logistic regression, Naive Bayes model it takes lesser time to train. This algorithm is perfect for use while working with multiple classes and text classification where the data is dynamic and changes frequently.\n"}
{"input": "Can you explain the concept of skewness in Descriptive Statistics?", "output": "Skewness measures the asymmetry of a probability distribution. If the data is skewed to the right, the tail is elongated towards larger values, and if it\u2019s skewed to the left, the tail is elongated towards smaller values. A perfectly symmetric distribution has a skewness of zero. Understanding skewness is important because it affects the choice of appropriate statistical methods and models."}
{"input": " What is DNS?", "output": "DNS is the Domain Name System. The main function of this network service is to provide host names to TCP/IP address resolution."}
{"input": "Write a simple code to binarize data.", "output": "Conversion of data into binary values on the basis of certain threshold is known as binarizing of data. Values below the threshold are set to 0 and those above the threshold are set to 1 which is useful for feature engineering.\n\nCode:\n\nfrom sklearn.preprocessing import Binarizer\n\nimport pandas\n\nimport numpy\n\nnames_list = ['Alaska', 'Pratyush', 'Pierce', 'Sandra', 'Soundarya', 'Meredith', 'Richard', 'Jackson', 'Tom',\u2019Joe\u2019]\n\ndata_frame = pandas.read_csv(url, names=names_list)\n\narray = dataframe.values\n\n# Splitting the array into input and output \n\nA = array [: 0:7]\n\nB = array [:7]\n\nbinarizer = Binarizer(threshold=0.0). fit(X)\n\nbinaryA = binarizer.transform(A)\n\nnumpy.set_printoptions(precision=5)\n\nprint (binaryA [0:7:])\n\nMachine Learning Using Python Interview Questions\n"}
{"input": "How would you use Python\u2019s Pandas library for time series analysis?", "output": "Pandas library in Python is a powerful tool for time series analysis. It provides functionalities like date range generation, frequency conversion, moving window statistics, date shifting and lagging. To use Pandas for time series analysis, we first import the library using \u201cimport pandas as pd\u201d. We can then create a DateTime index using \u201cpd.date_range\u201d, which generates sequences of fixed-frequency dates and time spans. This allows us to perform resampling operations during frequency conversion (downsampling or upsampling) with functions such as \u201cresample\u201d or \u201casfreq\u201d. For moving window statistics, we use rolling function. For example, \u201cdf.rolling(window=3).mean()\u201d calculates 3-day moving average. We can also shift or lag values back and forward in time using \u201cshift\u201d function. This is useful for calculating changes over time. In addition, Pandas supports slicing of data based on the date-time index, making it easy to slice and dice datasets according to our needs."}
{"input": "Which machine learning algorithm is known as the lazy learner, and why is it called so?", "output": "KNN is a Machine Learning algorithm known as a lazy learner. K-NN is a lazy learner because it doesn\u2019t learn any machine-learned values or variables from the training data but dynamically calculates distance every time it wants to classify, hence memorizing the training dataset instead. \n"}
{"input": " What is a LAN?", "output": "LAN stands for Local Area Network. It refers to the connection between computers and other network devices that are located within a small physical location."}
{"input": "How do you calculate the probability of independent events?", "output": "In the case of independent events, the probability of both events occurring is the product of their individual probabilities. For example, if the probability of flipping a coin and getting heads is 0.5, and the probability of rolling a dice and getting a 4 is 1/6, the probability of both events occurring together is 0.5 * 1/6 = 1/12."}
{"input": " What is a microprocessor?", "output": "A microprocessor is an integrated circuit having all the functionality of a central processing unit of a PC."}
{"input": "What is Feature Extraction in NLP?", "output": "Features or characteristics of a word help in text or document analysis. They also help in sentiment analysis of a text. Feature extraction is one of the techniques that are used by recommendation systems. Reviews such as \u2018excellent,\u2019 \u2018good,\u2019 or \u2018great\u2019 for a movie are positive reviews, recognized by a recommender system. The recommender system also tries to identify the features of the text that help in describing the context of a word or a sentence. Then, it makes a group or category of the words that have some common characteristics. Now, whenever a new word arrives, the system categorizes it as per the labels of such groups."}
{"input": "What are the major tasks of NLP?", "output": "Translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation are few of the major tasks of NLP. Under unstructured data, there can be a lot of untapped information that can help an organization grow.\n"}
{"input": " What is the maximum length allowed for a UTP cable?", "output": "A single segment of UTP cable has an allowable length of 90 to 100 meters. This limitation can be overcome by using repeaters and switches."}
{"input": "  What can be considered as good passwords?", "output": "Good passwords are made up of not just letters, but by combining letters and numbers. A password that combines uppercase and lowercase letters is favorable than one that uses all upper case or all lower-case letters. Passwords must be not words that can easily be guessed by hackers, such as dates, names, favorites, etc. Longer passwords are also better than short ones."}
{"input": "What is Marginalisation? Explain the process.", "output": "Marginalisation is summing the probability of a random variable X given joint probability distribution of X with other variables. It is an application of the law of total probability.\n\nP(X=x) = \u2211YP(X=x,Y) \n\nGiven the joint probability P(X=x,Y), we can use marginalization to find P(X=x). So, it is to find distribution of one random variable by exhausting cases on other random variables.\n"}
{"input": "What are ensemble models? Explain how ensemble techniques yield better learning as compared to traditional classification ML algorithms.", "output": "An ensemble is a group of models that are used together for prediction both in classification and regression classes. Ensemble learning helps improve ML results because it combines several models. By doing so, it allows for a better predictive performance compared to a single model. \n\nThey are superior to individual models as they reduce variance, average out biases, and have lesser chances of overfitting.\n"}
{"input": "What does a conditional probability mean in Bayesian statistics?", "output": "A conditional probability is the probability of an event occurring, given that another event has already occurred. For example, the probability of it raining tomorrow, given that it rained today."}
{"input": "If there is a 30 percent probability that you will see a supercar in any 20-minute time interval, what is the probability that you see at least one supercar in the period of an hour (60 minutes)?", "output": "The probability of not seeing a supercar in 20 minutes is: = 1 - P(Seeing one supercar) = 1 - 0.3. Probability of not seeing any supercar in the period of 60 minutes is: = (0.7) ^ 3 = 0.343. Hence, the probability of seeing at least one supercar in 60 minutes is: = 1 - P(Not seeing any supercar) = 1 - 0.343 = 0.657"}
{"input": "What is subroutine?", "output": "A subroutine is a self-contained set of statements that can be used from anywhere in a program. The subroutine performs its specific task and then returns control to the program that calls the subroutine."}
{"input": "  What is a queue?", "output": "A queue is a data structure that can simulate a list or stream of data. In this structure, new elements are inserted at one end, and existing elements are removed from the other end."}
{"input": "What is the meaning of degrees of freedom (DF) in statistics?", "output": "Degrees of freedom or DF is used to define the number of options at hand when performing an analysis. It is mostly used with t-distribution and not with the z-distribution. If there is an increase in DF, the t-distribution will reach closer to the normal distribution. If DF > 30, this means that the t-distribution at hand is having all of the characteristics of a normal distribution."}
{"input": "What is the difference between the first quartile, the second quartile, and the third quartile?", "output": "The first quartile is denoted by Q1 and it is the median of the lower half of the data set. The second quartile is denoted by Q2 and is the median of the data set. The third quartile is denoted by Q3 and is the median of the upper half of the data set. About 25% of the data set lies above Q3, 75% lies below Q3 and 50% lies below Q2. The Q1, Q2, and Q3 are the 25th, 50th, and 75th percentile respectively."}
{"input": "Can you explain the concept of autocorrelation and partial autocorrelation functions?", "output": "Autocorrelation, also known as serial correlation, is a statistical property where data points in a time series are correlated with others that are separated by a specific interval. It measures the linear relationship between lagged values of a variable. For instance, if an increase in sales in one month often results in an increase the following month, there\u2019s autocorrelation. Partial autocorrelation function (PACF), on the other hand, gives the partial correlation of a time series with its own lagged values, controlling for the values of the time series at all shorter lags. It contrasts with the autocorrelation function, which does not control for other lags. These concepts are crucial in identifying patterns and trends in time-series data, and they\u2019re fundamental to many forecasting methods, including ARIMA models."}
{"input": " Define String constants.", "output": "A string constant is a sequence of alphanumeric characters enclosed in double quotation marks. The maximum length of a string constant is 255 characters. For example, 'New York.'"}
{"input": "What are the variants of gradient descent?", "output": "There are three variants of gradient descent as shown below: \n Stochastic gradient descent: A single training example is used for the calculation of gradient and for updating parameters. \n Batch gradient descent: Gradient is calculated for the entire dataset, and parameters are updated at every iteration. \n Mini-batch gradient descent: Samples are broken down into smaller-sized batches and then worked on as in the case of stochastic gradient descent."}
{"input": "Can you explain the concept of exponential smoothing, and how it can be used for time series forecasting?", "output": "Exponential smoothing is a technique for forecasting time series data that assigns exponentially decreasing weights to past observations. The basic idea is to give more weight to recent observations, while still considering past observations in the forecast. This allows the forecast to adapt to changes in the data over time, while still capturing the underlying trends and patterns. The simplest form of exponential smoothing is called \u201csingle exponential smoothing,\u201d which is used to forecast time series data with no trend or seasonality. The formula for single exponential smoothing is: F_t = \u03b1Y_t + (1 \u2014 \u03b1)F_{t-1} where F_t is the forecast for time period t, Y_t is the observed value at time t, F_{t-1} is the forecast for the previous time period, and \u03b1 is the smoothing parameter, which controls the weight assigned to the most recent observation. The value of \u03b1 is usually between 0 and 1, with higher values giving more weight to recent observations."}
{"input": "What is Kernel Trick in an SVM Algorithm?", "output": "Kernel Trick is a mathematical function which when applied on data points, can find the region of classification between two different classes. Based on the choice of function, be it linear or radial, which purely depends upon the distribution of data, one can build a classifier. \n"}
{"input": "What are the analytical challenges during model development?", "output": "Non linearity \u2014 relationship between X and Y is non-linear. Hence difficult to model \u00b7 Model selection \u2014 selected the most accurate model but it may be an over-fit"}
{"input": "What is the difference between one hot encoding and ordinal encoding?", "output": "One Hot encoding and ordinal encoding both are different methods to convert categorical features to numeric ones the difference is in the way they are implemented. In one hot encoding, we create a separate column for each category and add 0 or 1 as per the value corresponding to that row. Contrary to one hot encoding, In ordinal encoding, we replace the categories with numbers from 0 to n-1 based on the order or rank where n is the number of unique categories present in the dataset. The main difference between one-hot encoding and ordinal encoding is that one-hot encoding results in a binary matrix representation of the data in the form of 0 and 1, it is used when there is no order or ranking between the dataset whereas ordinal encoding represents categories as ordinal values."}
{"input": "If a distribution is skewed to the right and has a median of 20, will the mean be greater than or less than 20?", "output": "If the given distribution is a right-skewed distribution, then the mean should be greater than 20, while the mode remains to be less than 20."}
{"input": "What is an Array?", "output": "An array is a collection of contiguous memory locations which can store data of the same type."}
{"input": "List the components of Natural Language Processing.", "output": "The major components of NLP are as follows: \n Entity extraction: Entity extraction refers to the retrieval of information such as place, person, organization, etc. by the segmentation of a sentence. It helps in the recognition of an entity in a text. Syntactic analysis: Syntactic analysis helps draw the specific meaning of a text. Pragmatic analysis: To find useful information from a text, we implement pragmatic analysis techniques. Morphological and lexical analysis: It helps in explaining the structure of words by analyzing them through parsing."}
{"input": "In the context of Hyperparameter optimization, explain the difference between grid search and random search.", "output": "In grid search, a set of pre-determined values is selected by a user for each dimension in his search space, and then thoroughly attempting each and every combination. Naturally, with such a large search space the number of the required combinations that need to be evaluated scale exponentially in the number of dimensions in the grid search. In random search the main difference is that the algorithm samples completely random points for each of the dimensions in the search space. Random search is usually faster and may even produce better results."}
{"input": "True or False: Considering a binary classification problem (y = 0 or y = 1), ensemble averaging, wherein the outputs of individual models are linearly combined to produce a fused output is a form of a static committee machine.", "output": "False. By definition, static committee machines use only the output of the single predictors."}
{"input": "What do you mean by overlays in OS?", "output": "Overlays is basically a programming method that divides processes into pieces so that instructions that are important and need can be saved in memory. It does not need any type of support from the OS. It can run programs that are bigger in size than physical memory by only keeping only important data and instructions that can be needed at any given time. "}
{"input": "What is an endless loop?", "output": "An endless loop can mean two things. One is that it was designed to loop continuously until the condition within the loop is met, after which a break function would cause the program to step out of the loop. Another idea of an endless loop is when an incorrect loop condition was written, causing the loop to run erroneously forever. Endless loops are oftentimes referred to as infinite loops."}
{"input": " Which cable is also called as the RG8 cable?", "output": "Thicknet cable is also called as the RG8 cable."}
{"input": "In what situation would the median be a more suitable measure compared to the mean?", "output": "In situations where outliers can affect data in either a positive or negative manner, the median is preferable due to its ability to accurately gauge this."}
{"input": "How can hyperparameters be trained in neural networks?", "output": "Hyperparameters can be trained using four components as shown below: \n Batch size: This is used to denote the size of the input chunk. Batch sizes can be varied and cut into sub-batches based on the requirement. \n Epochs: An epoch denotes the number of times the training data is visible to the neural network so that it can train. Since the process is iterative, the number of epochs will vary based on the data. \n Momentum: Momentum is used to understand the next consecutive steps that occur with the current data being executed at hand. It is used to avoid oscillations when training. \n Learning rate: Learning rate is used as a parameter to denote the time required for the network to update the parameters and learn."}
{"input": "What is the importance of regularisation? ", "output": "Regularisation is a technique that can help alleviate the problem of overfitting a model. It is beneficial when a large number of parameters are present, which help predict the target function. In these circumstances, it is difficult to select which features to keep manually.  Regularisation essentially involves adding coefficient terms to the cost function so that the terms are penalized and are small in magnitude. This helps, in turn, to preserve the overall trends in the data while not letting the model become too complex. These penalties, in effect, restrict the influence a predictor variable can have over the target by compressing the coefficients, thereby preventing overfitting."}
{"input": "Are there alternatives to find optimum parameters for logistic regression besides using Gradient Descent?", "output": "Yes, Gradient Descent is merely one of the many available optimization algorithms. Other advanced optimization algorithms can often help arrive at the optimum parameters faster and help with scaling for significant machine learning problems. A few such algorithms are Conjugate Gradient, BFGS, and L-BFGS algorithms."}
{"input": "Which cable uses the RJ11 connector?", "output": "Most of the telephone cable uses the RJ11 connector."}
{"input": "Can you mention some advantages and disadvantages of decision trees?", "output": "The advantages of decision trees are that they are easier to interpret, are nonparametric and hence robust to outliers, and have relatively few parameters to tune.\n\nOn the other hand, the disadvantage is that they are prone to overfitting.\n"}
{"input": "What is the thread in programming?", "output": "The thread is a fundamental concept in programming. It is a way of organizing and managing code so that it can run in parallel. The thread is a way of keeping code running in the background while the main program is running. Threads are used to parallelize tasks and to share resources among multiple threads. When the main program runs, it sends instructions to the thread that is running the task. When the task is done, the thread that was running the task sends instructions to the main program that is waiting for instructions. This process continues until all tasks are done or until one of them crashes or gets interrupted. Threads are a fundamental concept in programming because they allow programs to run in parallel and to share resources. They are also useful for managing memory and CPU usage."}
{"input": "How might root cause analysis be applied to a real-life situation?", "output": "The technique of identifying the source of a problem by identifying the root cause is known as root cause analysis. Examples: A positive correlation between the higher crime rate in a city and the higher sales of red shirts can be inferred from the above sentence. However, this does not mean that one causes the other. Correlational and experimental approaches can always be used to test causation."}
{"input": "Is the vertical offset, horizontal offset, or the perpendicular offset minimized for least-square fitting, assuming that the vertical axis is the dependent variable? Why is this so?", "output": "In most cases, the vertical offsets from a line (or a surface) are minimized instead of the perpendicular offsets (or the horizontal offset). The resultant fitting function for the independent variables that predict the dependent variable allows uncertainties of the data points to be represented in a simple manner. Further, compared to a fit based on perpendicular offsets, this practice allows for a much simpler analytic form for the fitting parameters."}
{"input": " How does dynamic loading aid in better memory space utilization?", "output": "With dynamic loading, a routine is not loaded until it is called. This method is especially useful when large amounts of code are needed in order to handle infrequently occurring cases such as error routines."}
{"input": "What is Data abstraction?", "output": "Data abstraction is a powerful tool for breaking down complex data problems into manageable chunks. This is applied by initially specifying the data objects involved and the operations to be performed on these data objects without being overly concerned with how the data objects will be represented and stored in memory."}
{"input": "Can you define Type I and Type II error?", "output": "Type I error can be defined as P Rejecting H0|H0 is True) which is false positive (where \u237a is defined as one minus the confidence level) and Type II error is defined to be P (Not Rejecting H0|H0 is False) (where \u03b2, is defined as one minus statistical power) and false negative. However, there can be a slight trade-off between both Type I and Type II errors. This simply means if you wish to decrease Type I error, you\u2019ll probably have to increase Type II error."}
{"input": "What are collinearity and multicollinearity?", "output": "Collinearity is a linear association between two predictors. Multicollinearity is a situation where two or more predictors are highly linearly related.\n"}
{"input": "What is NOS?", "output": "NOS is short for Network Operating System. It is a specialized software that will allow a computer to communicate with other devices over the network, including file/folder sharing."}
{"input": "What is NES?", "output": "Name entity recognition is more commonly known as NER is the process of identifying specific entities in a text document that are more informative and have a unique context. These often denote places, people, organizations, and more. Even though it seems like these entities are proper nouns, the NER process is far from identifying just the nouns. In fact, NER involves entity chunking or extraction wherein entities are segmented to categorize them under different predefined classes. This step further helps in extracting information. \n"}
{"input": "What are cherry-picking, P-hacking, and significance chasing?", "output": "Cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it. P-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect. Reporting insignificant results as if they are almost significant, is known as Significance Chasing. Data Dredging, Data Fishing, and Data Snooping are all names for this behaviour."}
{"input": "What is F1 score in NLP?", "output": "F1 score evaluates the weighted average of recall and precision. It considers both false negative and false positive instances while evaluating the model. F1 score is more accountable than accuracy for an NLP model when there is an uneven distribution of class. Let us look at the formula for calculating F1 score: F_1 = 2 * (precision * recall)/(precision + recall)"}
{"input": "What is the importance of Encryption on a network?", "output": "Encryption is the process of translating information into a code that is unreadable by the user. It is then translated back or decrypted back to its normal readable format using a secret key or password. Encryption ensures that information that is intercepted halfway would remain unreadable because the user must have the correct password or key for it."}
{"input": "What is the difference between Shallow Networks and Deep Networks?", "output": "Deep networks and shallow networks are two types of artificial neural networks that can learn from data and perform tasks such as classification, regression, clustering, and generation. \n Shallow networks: A shallow network has a single hidden layer between the input and output layers, whereas a deep network has several hidden layers. Because they have fewer parameters, they are easier to train and less computationally expensive than deep networks. Shallow networks are appropriate for basic or low-complexity tasks where the input-output relationships are relatively straightforward and do not require extensive feature representation. \n Deep Networks: Deep networks, also known as deep neural networks, can be identified by the presence of many hidden layers between the input and output layers. The presence of multiple layers enables deep networks to learn hierarchical data representations, capturing detailed patterns and characteristics at different levels of abstraction. It has a higher capacity for feature extraction and can learn more complex and nuanced relationships in the data. It has given state-of-the-art results in many machine learning and AI tasks."}
{"input": "Which of the following techniques can be used for keyword normalization in NLP, the process of converting a keyword into its base form? a. Lemmatization b. Soundex c. Cosine Similarity d. N-grams", "output": "Answer: a)\n\nLemmatization helps to get to the base form of a word, e.g. are playing -> play, eating -> eat, etc. Other options are meant for different purposes.\n"}
{"input": "What is linear regression? ", "output": "In statistics, linear regression is an approach that models the relationship between one or more explanatory variables and one outcome variable. For example, linear regression can be used to quantify or model the relationship between various predictor variables such as age, gender, genetics, and diet on height, outcome variables. "}
{"input": "Explain three different types of sampling techniques.", "output": "Simple random sampling: The individual is selected from the true population entirely by chance, and every individual has an equal opportunity to get selected. Stratified sampling: The population is first divided into multiple strata that share similar characteristics, and each strata is sampled in equal sizes. This is done to ensure equal representation of all sub-groups. Systematic sampling: Individuals are selected from the sampling frame at regular intervals. For example, every 10th member is selected from the sampling frame. This is one of the easiest sampling techniques but can introduce bias into the sample population if there is an underlying pattern in the true population."}
{"input": " What Is A Super-class?", "output": "A super class is the basis of all the classes. The object of the rest of the class has all the characteristics related to the superclass."}
{"input": "How would you evaluate a logistic regression model?", "output": "Model Evaluation is a very important part in any analysis to answer the following questions,\n\nHow well does the model fit the data?, Which predictors are most important?, Are the predictions accurate?\n\nSo the following are the criterion to access the model performance,\n\nAkaike Information Criteria (AIC): In simple terms, AIC estimates the relative amount of information lost by a given model. So the less information lost the higher the quality of the model. Therefore, we always prefer models with minimum AIC.\n\nReceiver operating characteristics (ROC curve): ROC curve illustrates the diagnostic ability of a binary classifier. It is calculated/ created by plotting True Positive against False Positive at various threshold settings. The performance metric of ROC curve is AUC (area under curve). Higher the area under the curve, better the prediction power of the model.\n\nConfusion Matrix: In order to find out how well the model does in predicting the target variable, we use a confusion matrix/ classification rate. It is nothing but a tabular representation of actual Vs predicted values which helps us to find the accuracy of the model.\n"}
{"input": " Explain what is a recursive algorithm?", "output": "Recursive algorithm is a method of solving a complicated problem by breaking a problem down into smaller and smaller sub-problems until you get the problem small enough that it can be solved easily. Usually, it involves a function calling itself."}
{"input": "How do you handle multiple comparisons in inferential statistics?", "output": "When conducting multiple statistical tests simultaneously, the chance of obtaining false positives (Type I errors) increases. To control for this, we use methods like Bonferroni correction, False Discovery Rate (FDR), or the Benjamini-Hochberg procedure to adjust the significance level for each test, reducing the overall risk of making Type I errors."}
{"input": "What is the definition of an inlier?", "output": "An error instance is usually identified as an Inliner within a data set. It is usually a lower-level data point and should therefore be removed. Finding inliers is usually difficult and requires outside data to identify."}
{"input": "What is a Deep Learning framework?", "output": "A deep learning framework is a collection of software libraries and tools that provide programmers a better deep learning model development and training possibilities. It offers a high-level interface for creating and training deep neural networks in addition to lower-level abstractions for implementing special functions and topologies. TensorFlow, PyTorch, Keras, Caffe, and MXNet are a few of the well-known frameworks for deep learning."}
{"input": "When do you use a one-tailed (one-sided) test versus a two-tailed (two-sided) test in hypothesis testing?", "output": "In a one-tailed test, we are interested in whether the value of a parameter is significantly greater than or less than the hypothesized value. In a two-tailed test, we want to determine if the parameter significantly differs from the hypothesized value in any direction. The choice between the two depends on the research question and the specific hypothesis being tested."}
{"input": "In text mining, converting text into tokens and then converting them into an integer or floating-point vectors can be done using a. CountVectorizer b. TF-IDF c. Bag of Words d. NERs", "output": "Answer: a)\n\nCountVectorizer helps do the above, while others are not applicable.\n\ntext =['Rahul is an avid writer, he enjoys studying understanding and presenting. He loves to play']\n\nvectorizer = CountVectorizer()\n\nvectorizer.fit(text)\n\nvector = vectorizer.transform(text)\n\nprint(vector.toarray())\n\nOutput \n\n[[1 1 1 1 2 1 1 1 1 1 1 1 1 1]]\n\nThe second section of the interview questions covers advanced NLP techniques such as Word2Vec, GloVe word embeddings, and advanced models such as GPT, Elmo, BERT, XLNET-based questions, and explanations.\n"}
{"input": "Differentiate linear from a nonlinear data structure.", "output": "The linear data structure is a structure wherein data elements are adjacent to each other. Examples of linear data structure include arrays, linked lists, stacks, and queues. On the other hand, a non-linear data structure is a structure wherein each data element can connect to more than two adjacent data elements. Examples of nonlinear data structure include trees and graphs."}
{"input": "What are doubly linked lists?", "output": "Doubly linked lists are a special type of linked list wherein traversal across the data elements can be done in both directions. This is made possible by having two links in every node, one that links to the next node and another one that connects to the previous node."}
{"input": "What is the meaning of an inlier?", "output": "An inlier is a data point that lies at the same level as the rest of the dataset. Finding an inlier in the dataset is difficult when compared to an outlier as it requires external data to do so. Inliers, similar to outliers reduce model accuracy. Hence, even they are removed when they\u2019re found in the data. This is done mainly to maintain model accuracy at all times."}
{"input": "What are the assumptions of linear regression?", "output": "\u00b7 Linearity of independent and dependent variable \u00b7 Errors should be normally distributed with mean of zero \u00b7 Errors have equal variance \u00b7 Errors are independent"}
{"input": "What are the assumptions of the ordinary least squares method for linear regression?", "output": "The ordinary least squares method for linear regression makes several assumptions about the data and the relationship between the variables. These assumptions include: The dependent variable is continuous and normally distributed. The independent variables are linearly related to the dependent variable. There is no multicollinearity among the independent variables. The errors are independent and identically distributed. The errors have a mean of zero. These assumptions help to ensure that the resulting model is reliable and accurately describes the relationship between the variables. It's important to test these assumptions and ensure that they are met before using the model for prediction."}
{"input": "What is the relationship between mean and median in normal distribution?", "output": "In a normal distribution, the mean and the median are equal."}
{"input": "What is an Algorithm?", "output": "An algorithm is a rule or step-by-step process that must be followed in order to solve a particular problem."}
{"input": "What is the relationship between standard error and the margin of error?", "output": "The margin of error is proportionally influenced by the standard error. In other words, the margin of error is computed using standard error. As standard error increases, the margin of error also rises."}
{"input": "True or false: In bagging, we re-sample the training corpus with replacement and therefore this may lead to some instances being represented numerous times while other instances not to be represented at all.", "output": "This is true. In a bagging approach, we first randomly draw (with replacement), K examples where K is the size of the original training corpus therefore leading to an imbalanced representation of the instances."}
{"input": "What are hyperparameters?", "output": "Hyperparameters are created from prior observation before a dataset is captured and used in deep learning algorithms to train a model. Hyperparameters need to be initialised before training a model. The benefits of using hyperparameters are that they can control the behaviour of a training model and impact its performance significantly.\n\nHyperparameters can be optimised by Grid search, Random search, and Bayesian optimisation. Choosing good hyperparameters ensure easy management of large datasets.\n"}
{"input": "When would you use ensemble methods and why?", "output": "Ensemble methods are used when a single model\u2019s predictive performance is insufficient. They combine multiple models to improve accuracy and robustness, reducing both bias and variance. This method is particularly useful in complex problems where data may be noisy or contain outliers. Ensemble techniques like bagging help reduce overfitting by averaging predictions from various models, while boosting focuses on training instances that previous models handled poorly, improving overall prediction. Stacking combines different types of models, leveraging their individual strengths."}
{"input": " Define Numeric constants.", "output": "Numeric constants consist of integers, single precision, or double-precision numbers. Integer constants represent values that are counted and do not have a fractional part, e.g., +56, -678"}
{"input": " Describe Banker\u2019s algorithm", "output": "Banker\u2019s algorithm is one form of deadlock-avoidance in a system. It gets its name from a banking system wherein the bank never allocates available cash in such a way that it can no longer satisfy the needs of all of its customers."}
{"input": "What is cryptography?", "output": "Cryptography is the study of techniques that hide the real meaning of information. It transforms this information into a format that cannot be read by humans and vice versa."}
{"input": "What protocols fall under the Application layer of the TCP/IP stack?", "output": "The following are the protocols under the TCP/IP Application layer: FTP, TFTP, Telnet, and SMTP."}
{"input": "True or False: The KL-divergence satisfies the triangle inequality.", "output": "False KLD does not satisfy the triangle inequality."}
{"input": "What is plumbing/piping?", "output": "It is the process of using the output of one program as an input to another. For example, instead of sending the listing of a folder or drive to the main screen, it can be piped and sent to a file, or sent to the printer to produce a hard copy."}
{"input": "Which metrics can be used to measure correlation of categorical data?", "output": "Ans. Chi square test can be used for doing so. It gives the measure of correlation between categorical predictors.\n"}
{"input": " What is data structure?", "output": "Data structure refers to the way data is organized and manipulated. It seeks to find ways to make data access more efficient. When dealing with the data structure, we not only focus on one piece of data but the different set of data and how they can relate to one another in an organized manner."}
{"input": " What is Tensorflow, and What is It Used For?", "output": "TensorFlow is an open-source software library initially developed by the Google Brain Team for use in machine learning and neural networks research. It is used for data-flow programming. TensorFlow makes it much easier to build certain AI features into applications, including natural language processing and speech recognition."}
{"input": "What is the difference between linear regression and non-linear regression?", "output": "Linear regression is a statistical method that uses a linear equation to model the relationship between a dependent variable and one or more independent variables. The linear equation is of the form y = mx + b, where y is the dependent variable, x is the independent variable, m is the slope of the line, and b is the y-intercept. In linear regression, the relationship between the variables is assumed to be linear, meaning that the dependent variable changes at a constant rate with respect to the independent variable. Non-linear regression, on the other hand, is a statistical method that uses a non-linear equation to model the relationship between a dependent variable and one or more independent variables. The non-linear equation can be of many different forms, including polynomial equations, exponential equations, and logarithmic equations. In non-linear regression, the relationship between the variables is not assumed to be linear, meaning that the dependent variable can change at different rates with respect to the independent variable."}
{"input": "Which sampling technique is most suitable when working with time-series data?", "output": "Ans. We can use a custom iterative sampling such that we continuously add samples to the train set. We only should keep in mind that the sample used for validation should be added to the next train sets and a new sample is used for validation.\n"}
{"input": "What are the different different types of activation functions used in deep learning?", "output": "In deep learning, several different-different types of activation functions are used. Each of them has its own strength and weakness. Some of the most common activation functions are as follows. \n Sigmoid function: It maps any value between 0 and 1. It is mainly used in binary classification problems. where it maps the output of the preceding hidden layer into the probability value. \n Softmax function: It is the extension of the sigmoid function used for multi-class classification problems in the output layer of the neural network, where it maps the output of the previous layer into a probability distribution across the classes, giving each class a probability value between 0 and 1 with the sum of the probabilities over all classes is equal to 1. The class which has the highest probability value is considered as the predicted class. \n ReLU (Rectified Linear Unit) function: It is a non-linear function that returns the input value for positive inputs and 0 for negative inputs. Deep neural networks frequently employ this function since it is both straightforward and effective. \n Leaky ReLU function: It is similar to the ReLU function, but it adds a small slope for negative input values to prevent dead neurons. \n Tanh (hyperbolic tangent) function: It is a non-linear activations function that maps the input\u2019s value between -1 to 1. It is similar to the sigmoid function but it provides both positive and negative results. It is mainly used for regression tasks, where the output will be continuous values."}
{"input": "Can you give an example of root cause analysis?", "output": "Root cause analysis, as the name suggests, is a method used to solve problems by first identifying the root cause of the problem. Example: If the higher crime rate in a city is directly associated with the higher sales in a red-colored shirt, it means that they are having a positive correlation. However, this does not mean that one causes the other. Causation can always be tested using A/B testing or hypothesis testing."}
{"input": " What is the advantage of address sharing?", "output": "By using address translation instead of routing, address sharing provides an inherent security benefit. That\u2019s because host PCs on the Internet can only see the public IP address of the external interface on the computer. Instead, it provides address translation and not the private IP addresses on the internal network."}
{"input": "What is HTML?", "output": "HTML stands for Hyper Text Markup Language. It is a language of World Wide Web. It is a standard text formatting language which is used to create and display pages on the Web. It makes the text more interactive and dynamic. It can turn text into images, tables, links. More details."}
{"input": "Can you describe a time when using Bayesian Inference led to a significantly different outcome than traditional statistical methods?", "output": "In a project involving customer behavior prediction, Bayesian Inference provided different results than traditional statistical methods. The task was to predict future purchases based on past data. Traditional statistics, using frequentist approach, gave us an average number of purchases without considering individual differences. However, with Bayesian Inference, we incorporated prior knowledge about customers\u2019 buying habits into our model. This allowed for more personalized predictions. For instance, if a customer had a history of making large purchases during holiday seasons, the Bayesian model would factor this in and predict higher purchase likelihood during these periods. This level of personalization led to more accurate predictions compared to the one-size-fits-all approach of traditional statistics."}
{"input": "Identify the odd one out a. nltk b. scikit learn c. SpaCy d. BERT", "output": "Answer: d)\n\nAll the ones mentioned are NLP libraries except BERT, which is a word embedding.\n"}
{"input": "What is the relationship between mean and median in a normal distribution?", "output": "In a normal distribution, the mean is equal to the median. To know if the distribution of a dataset is normal, we can just check the dataset's mean and median."}
{"input": "Please describe a real-world scenario where a time series analysis can be beneficial.", "output": "Time series analysis can be beneficial in the stock market. Investors and financial analysts often use it to predict future stock prices based on historical data. For instance, they may analyze a company\u2019s stock price over several years to identify trends or patterns that could indicate whether the price will rise or fall in the future. This information is crucial for making informed investment decisions. Similarly, economists might use time series analysis to forecast economic indicators like GDP or unemployment rates, helping policymakers make decisions about fiscal policy."}
{"input": "What are observational and experimental data in statistics?", "output": "Observational data is derived from the observation of certain variables from observational studies. The variables are observed to determine any correlation between them. Experimental data is derived from those experimental studies where certain variables are kept constant to determine any discrepancy or causality. "}
{"input": "Define the term fine-tuning (FT) of an ImageNet pre-trained CNN.", "output": "The term fine-tuning (FT) of an ImageNet pre-trained CNN refers to the method by which one or more of the weights of the CNN are re-trained on a new target data-set, which may or may-not have similarities with the ImageNet data-set. "}
{"input": "What is a Boltzmann machine?", "output": "A Boltzmann machine is a type of recurrent neural network that uses binary decisions, alongside biases, to function. These neural networks can be hooked up together to create deep belief networks, which are very sophisticated and used to solve the most complex problems out there."}
{"input": "What are the advantages of using a naive Bayes for classification?", "output": "Very simple, easy to implement and fast.\n\nIf the NB conditional independence assumption holds, then it will converge quicker than discriminative models like logistic regression.\n\nEven if the NB assumption doesn\u2019t hold, it works great in practice.\n\nNeed less training data.\n\nHighly scalable. It scales linearly with the number of predictors and data points.\n\nCan be used for both binary and mult-iclass classification problems.\n\nCan make probabilistic predictions.\n\nHandles continuous and discrete data.\n\nNot sensitive to irrelevant features.\n"}
{"input": "Can you explain the concept of \u201cwhite noise\u201d and its importance in time series analysis?", "output": "White noise in time series analysis is a sequence of random variables, each with zero mean, constant variance, and no correlation over time. It\u2019s crucial because it serves as the baseline model for randomness or unpredictability. In real-world data, deviations from white noise can indicate patterns, trends, or relationships that might otherwise go unnoticed. For instance, if residuals from a model resemble white noise, it suggests the model has adequately captured the information in the data. Conversely, non-white noise may signal missed predictive opportunities."}
{"input": "In classification problems like logistic regression, classification accuracy alone is not considered a good measure. Why?", "output": "Classification accuracy considers both true positives and false positives with equal significance. If this were just another machine learning problem of not too much consequence, this would be acceptable. However, when the problems involve deciding whether to consider a candidate for life-saving treatment, false positives might not be as bad as false negatives. The opposite can also be true in some cases. Therefore, while there is no single best way to evaluate a classifier, accuracy alone may not serve as a good measure."}
{"input": "How are covariance and correlation different from one another?", "output": "Covariance measures how two variables are related to each other and how one would vary with respect to changes in the other variable. If the value is positive it means there is a direct relationship between the variables and one would increase or decrease with an increase or decrease in the base variable respectively, given that all other conditions remain constant.\tCorrelation quantifies the relationship between two random variables and has only three specific values, i.e., 1, 0, and -1.\n\n1 denotes a positive relationship, -1 denotes a negative relationship, and 0 denotes that the two variables are independent of each other.\n"}
{"input": "What are the hyperparameters of a logistic regression model?", "output": "Ans. Classifier penalty, classifier solver and classifier C are the trainable hyperparameters of a Logistic Regression Classifier. These can be specified exclusively with values in Grid Search to hyper tune a Logistic Classifier.\n"}
{"input": "How do you identify outliers in a dataset using Descriptive Statistics?", "output": "Outliers are extreme values that deviate significantly from the rest of the data. One way to identify outliers is by calculating the Z-score of each data point, which measures how many standard deviations it is away from the mean. Data points with Z-scores beyond a certain threshold (e.g., \u00b13) are considered outliers."}
{"input": "Please define hypothesis testing?", "output": "Hypothesis testing can be defined as a method of statistical inference out of which you calculate the probability (p-value) of observing the statistics from the given data and conclude only if the null hypothesis is true. Now based on this you would have to decide whether or not you need to reject the null hypothesis by comparing the p-value and the significance level. The testing is majorly used for testing the existence of an effect."}
{"input": "What is a Critical Section?", "output": "A critical section is a section of code that is crucial to the operation of a program. By accessing critical sections, a program can make changes to the overall functionality of the program without affecting other sections of code. Critical sections are often accessed through functions, methods, and variables. The purpose of a critical section is to ensure that all parts of the program are working properly."}
{"input": "Complete the sentence: Bagging is variance/bias reduction scheme while boosting reduced variance/bias.", "output": "Bagging is variance reduction scheme while boosting reduced bias."}
{"input": "Model accuracy or Model performance? Which one will you prefer and why?", "output": "This is a trick question, one should first get a clear idea, what is Model Performance? If Performance means speed, then it depends upon the nature of the application, any application related to the real-time scenario will need high speed as an important feature. Example: The best of Search Results will lose its virtue if the Query results do not appear fast.\n\nIf Performance is hinted at Why Accuracy is not the most important virtue \u2013 For any imbalanced data set, more than Accuracy, it will be an F1 score than will explain the business case and in case data is imbalanced, then Precision and Recall will be more important than rest.\n"}
{"input": "Same word can have multiple word embeddings possible with ____________? a. GloVe b. Word2Vec c. ELMo d. nltk", "output": "Answer: c)\n\nEMLo word embeddings support the same word with multiple embeddings, this helps in using the same word in a different context and thus captures the context than just the meaning of the word unlike in GloVe and Word2Vec. Nltk is not a word embedding.\n"}
{"input": "Complete the sentence: Due to the time-consuming nature of computing gradients for each entry in the training corpus, modern DL libraries utilize a technique that gauges the gradient by first randomly sampling a subset from the training corpus, and then averaging only this subset in every epoch. This approach is known as [...]. The actual number of randomly chosen samples in each epoch is termed [...]. The gradient itself is obtained by an algorithm known as [...].", "output": "Due to the time-consuming nature of computing gradients for each entry in the training corpus, modern DL libraries utilize a technique that gauges the gradient by first randomly sampling a subset from the training corpus, and then averaging only this subset in every epoch. This approach is known as Stochastic gradient descent. The actual number of randomly chosen samples in each epoch is termed Batch size. The gradient itself is obtained by an algorithm known as Back-propagation."}
{"input": "Is a high variance in data good or bad?", "output": "Higher variance directly means that the data spread is big and the feature has a variety of data. Usually, high variance in a feature is seen as not so good quality.\n"}
{"input": " What is the meaning of \u201cexport\u201d command in Ubuntu?", "output": "Export is a command in Bash shell language. When you try to set a variable, it is visible or exported to any subprocess started from that instance of bash.  The variable will not exist in the sub-process without the export command."}
{"input": "What is mesh topology?", "output": "Mesh topology is a setup wherein each device is connected directly to every other device on the network. Consequently, it requires that each device has at least two network connections."}
{"input": " What are MAC addresses?", "output": "MAC, or Media Access Control, uniquely identifies a device on the network. It is also known as a physical address or an Ethernet address. A MAC address is made up of 6-byte parts."}
{"input": "Explain the differences between Random Forest and Gradient Boosting machines.", "output": "Random forests are a significant number of decision trees pooled using averages or majority rules at the end.\tGradient boosting machines also combine decision trees but at the beginning of the process, unlike Random forests.\n\nThe random forest creates each tree independent of the others while gradient boosting develops one tree at a time.\tGradient boosting yields better outcomes than random forests if parameters are carefully tuned but it\u2019s not a good option if the data set contains a lot of outliers/anomalies/noise as it can result in overfitting of the model.\n\nRandom forests perform well for multiclass object detection.\tGradient Boosting performs well when there is data which is not balanced such as in real-time risk assessment.\n"}
{"input": " Why do we need to do an algorithm analysis? ", "output": "A problem can be solved in more than one way using several solution algorithms. Algorithm analysis provides an estimation of the required resources of an algorithm to solve a specific computational problem. The amount of time and space resources required to execute is also determined.The time complexity of an algorithm quantifies the amount of time taken for an algorithm to run as a function of the length of the input. The space complexity quantifies the amount of space or memory taken by an algorithm, to run as a function of the length of the input."}
{"input": "It is common practice that when the number of features or independent variables is larger in comparison to the training set, it is common to use logistic regression or support vector machine (SVM) with a linear kernel. What is the reasoning behind this?", "output": "It is common to use logistic regression or SVM with a linear kernel because when there are many features with a limited number of training examples, a linear function should be able to perform reasonably well. Besides, there is not enough training data to allow for the training of more complex functions."}
{"input": "What's the difference between interrupt service routine and subroutine?", "output": "Flip-flops, also called 'latches', are electronic circuits that have two stable states used to store binary data. The data stored in the states can be modified by using varying inputs. Flip-flops are fundamental components of digital electronic systems used in communications, computers and many other kinds of systems."}
{"input": "What is the equation for confidence intervals for means vs for proportions?", "output": "To calculate the confidence intervals for mean, we use the following equation \u2013 For n > 30, Use the Z table for the standard normal distribution. For n<30, Use the t table with df=n-1"}
{"input": "We look at machine learning software almost all the time. How do we apply Machine Learning to Hardware?", "output": "We have to build ML algorithms in System Verilog which is a Hardware development Language and then program it onto an FPGA to apply Machine Learning to hardware.\n"}
{"input": "How would you implement a Bayesian model in Python or R?", "output": "In Python, the PyMC3 library is ideal for implementing a Bayesian model. First, import necessary libraries: pandas for data manipulation and PyMC3 for Bayesian inference. Load your dataset using pandas\u2019 read_csv function. Define your model within a \u2018with\u2019 statement block. For instance, if predicting sales based on advertising spend, define priors for unknown parameters within the model context. These could be normal distributions for intercept and slope, and half-normal for standard deviation of observations. Next, specify likelihood which is assumed to be normally distributed with a mean that\u2019s a linear function of predictors. Use pm.Normal() for this. Finally, perform Markov Chain Monte Carlo sampling using pm.sample(). In R, use the brms package following similar steps but replace PyMC3 functions with equivalent brms ones."}
{"input": "If you observe that the test error is increasing after a certain number of iterations, what do you infer is most likely to be occurring? How do you address this problem?", "output": "Observing an increase in error on the validation set after a certain number of iterations can indicate that the model is Overfitting. We can arrive at this diagnosis because we expect the error to decrease with more optimized parameters. While simplifying the model is one way to address this problem, early stopping is another commonly used solution. Early stopping is probably one of the most commonly used forms of regularization. Unlike a weight decay used in the cost function, which helps to arrive at less complex models by explicit regularization, early stopping can be considered as a form of implicit regularization."}
{"input": "What is NOS?", "output": "NOS, or Network Operating System, is specialized software. The main task of this software is to provide network connectivity to a computer in order to communicate with other computers and connected devices."}
{"input": "What does standard deviation mean?", "output": "When a set of data points is near the mean, a low value of standard deviation indicates that the points are close to the mean, and a high value indicates that the points are far away from the mean. On the other hand, when the data points are far apart from each other, a high standard deviation indicates that the points are far away from the mean, and a low standard deviation indicates that the points are close to the mean."}
{"input": "What is the meaning of selection bias?", "output": "Selection bias is a phenomenon that involves the selection of individual or grouped data in a way that is not considered to be random. Randomization plays a key role in performing analysis and understanding model functionality better. If correct randomization is not achieved, then the resulting sample will not accurately represent the population."}
{"input": "What are the cons of Naive Bayes classifier?", "output": "Naive Bayes classifiers suffer from \u201cZero Frequency\u201d problem. This happens when a category is not present in the training set. It will give it 0 probability.  Its biggest downside is the consideration of features as independent of each other because in real life it is impossible to get independent features. All features are somehow co-related with each other. "}
{"input": "What is the empirical rule?", "output": "In statistics, the empirical rule states that every piece of data in a normal distribution lies within three standard deviations of the mean. It is also known as the 68\u201395\u201399.7 rule. According to the empirical rule, the percentage of values that lie in a normal distribution follow the 68%, 95%, and 99.7% rule. In other words, 68% of values will fall within one standard deviation of the mean, 95% will fall within two standard deviations, and 99.75 will fall within three standard deviations of the mean."}
{"input": "What\u2019s a Fourier transform?", "output": "Fourier Transform is a mathematical technique that transforms any function of time to a function of frequency. Fourier transform is closely related to Fourier series. It takes any time-based pattern for input and calculates the overall cycle offset, rotation speed and strength for all possible cycles. Fourier transform is best applied to waveforms since it has functions of time and space. Once a Fourier transform applied on a waveform, it gets decomposed into a sinusoid.\n\nCheck out the detailed eligibility requirements for the different ML courses offered by Great Learning\n"}
{"input": "You need to connect two computers for file sharing. Is it possible to do this without using a hub or a router?", "output": "Yes, you can connect two computers, using only one cable. A crossover type cable can be used in this scenario. In this setup, the data transmit pin of one cable is connected to the data receive pin of the other cable, and vice versa."}
{"input": "In your training loop, you are using SGD and a logistic activation function which is known to suffer from the phenomenon of saturated units. Explain the phenomenon.", "output": "The derivative of the logistic activation function is extremely small for either negtive or positive large inputs."}
{"input": "Can you give an example of a non-informative prior? How do they impact the outcome of Bayesian analysis?", "output": "A non-informative prior, also known as a flat or uninformative prior, is one where the probabilities are distributed equally across all possible outcomes. An example of this would be a uniform distribution over an interval [a,b], where every point in that interval has an equal chance of being selected. In Bayesian analysis, priors represent our existing knowledge before observing any data. Non-informative priors are used when we have no prior information about the parameter of interest. They allow the data to speak for itself and do not bias the posterior towards any particular values. However, it\u2019s important to note that even so-called \u201cnon-informative\u201d priors can still introduce some form of bias into the analysis, especially if they\u2019re improperly applied. For instance, using a uniform prior over an infinite range could lead to improper integrals in the posterior distribution. Therefore, careful consideration must be given when choosing priors in Bayesian analysis."}
{"input": "Can you explain the concept of time series decomposition?", "output": "Time series decomposition is a method for separating a time series into its underlying trend, seasonal, and residual components. This can be useful for understanding the patterns and drivers of variation in the data, as well as for improving the accuracy of time series forecasts."}
{"input": "Explain the bagging technique in random forest models.", "output": "Bagging stands for bootstrap aggregation. In random forests, the dataset is sampled multiple times using the bootstrap technique with replacement. Weak learners are trained independently on each sample with different features to split on at each node. Finally, the average or majority class prediction is provided as output to the user."}
{"input": "If you observe that the cost function decreases rapidly before increasing or stagnating at a specific high value, what could you infer?", "output": "A trend pattern of the cost curve exhibiting a rapid decrease before then increasing or stagnating at a specific high value indicates that the learning rate is too high. The gradient descent is bouncing around the global minimum but missing it owing to the larger than necessary step size."}
{"input": "Explain the difference between Lasso and Ridge?", "output": "Lasso(L1) and Ridge(L2) are the regularization techniques where we penalize the coefficients to find the optimum solution. In ridge, the penalty function is defined by the sum of the squares of the coefficients and for the Lasso, we penalize the sum of the absolute values of the coefficients. Another type of regularization method is ElasticNet, it is a hybrid penalizing function of both lasso and ridge. \n"}
{"input": "What mathematical concept Naive Bayes is based on?", "output": "Naive Bayes is based on Bayes theorem in statistics. It calculates probabilities independently for each class based on conditions and without conditions and then predicts outcomes based on that."}
{"input": "Will the decision boundary be linear or non-linear in logistic regression models? Explain with an example.", "output": "The decision boundary is essentially a line or a plane that demarcates the boundary between the classes to which linear regression classifies the dependent variables. The shape of the decision boundary will depend entirely on the logistic regression model. For logistic regression model given by hypothesis function h(x)=g(Tx)where g is the sigmoid function, if the hypothesis function is h(x)=g(1+2x2+3x3)then the decision boundary is linear. Alternatively, if h(x)=g(1+2x22+3x32)then the decision boundary is non-linear."}
{"input": "Explain Non-Probability Sampling and provide a few examples?", "output": "The sampling technique which gives varied chances to the data values of the population to get selected into the sample is known as Non-probability sampling or Biased sampling. Examples of Biased Sampling are: Convenience Sampling, Quota Sampling, Judgment Sampling, Snowball Sampling"}
{"input": "What is IPC? What are the different IPC mechanisms?", "output": "IPC (Interprocess Communication) is a mechanism that requires the use of resources like a memory that is shared between processes or threads. With IPC, OS allows different processes to communicate with each other. It is simply used for exchanging data between multiple threads in one or more programs or processes. In this mechanism, different processes can communicate with each other with the approval of the OS."}
{"input": "What is standardization? Under which circumstances should data be standardized?", "output": "Standardization is the process of putting different variables on the same scale. Variables are made to follow a standard normal distribution with a mean of 0 and a standard deviation of 1. Standardizing data can give us a better idea of extreme outliers, as it is easy to identify values that are 2\u20133 standard deviations away from the mean. Standardization is also used as a pre-processing technique before feeding data into machine learning models so that all variables are given the same weightage."}
{"input": "Can you explain the concept of \u2018Bayesian Belief Networks\u2019?", "output": "Bayesian Belief Networks (BBNs), also known as Bayesian Networks, are graphical models that represent the probabilistic relationships among a set of variables. They use directed acyclic graphs where nodes represent variables and edges signify conditional dependencies. Each node has an associated probability function that takes inputs from its parent nodes and gives the probability distribution for the variable represented. The key advantage of BBNs is their ability to handle uncertainty effectively by updating probabilities based on new evidence through Bayes\u2019 theorem. This makes them useful in various fields like medical diagnosis, risk analysis, machine learning, etc., where dealing with uncertain situations is common."}
{"input": "What are the computational challenges you faced while implementing Bayesian Inference, and how did you overcome them?", "output": "Implementing Bayesian Inference presented several computational challenges. The most significant was the high dimensionality of data, which made computations complex and time-consuming. To overcome this, I used Markov Chain Monte Carlo (MCMC) methods that simplified calculations by sampling from a distribution. Another challenge was dealing with missing or incomplete data. This issue was addressed using multiple imputation techniques to fill in gaps, improving the robustness of our model. The third challenge was choosing an appropriate prior. Priors can be subjective and influence results significantly. I mitigated this by conducting sensitivity analysis to understand how different priors affect outcomes. Lastly, Bayesian inference requires substantial computational resources, especially for large datasets. To manage this, I leveraged cloud computing solutions that provided scalable resources on demand."}
{"input": " What is the Difference Between Eigenvalues and Eigenvectors?", "output": "Eigenvalues and eigenvectors are related mathematical concepts that are used in linear algebra and have applications in many fields, such as physics, engineering, and computer science.An eigenvalue is a scalar value that represents the amount of stretching or shrinking that occurs when a linear transformation is applied to a vector. In other words, it is a scalar that is multiplied to a non-zero vector by a linear operator (often represented by a square matrix) to give the same vector but scaled.An eigenvector, on the other hand, is a non-zero vector that, when multiplied by a linear operator, results in a scaled version of itself. In other words, it is a non-zero vector that when multiplied by a square matrix, gives the same vector but scaled by a scalar, that scalar is the eigenvalue."}
{"input": "Complete the sentence: In a single-layer feed-forward NN, there are [...] input(s) and [...]. output layer(s) and no [...] connections at all.", "output": "In a single-layer feed-forward NN, there are one input(s) and one. output layer(s) and no feedback connections at all."}
{"input": " What is the client/server?", "output": "Client/server is a type of network wherein one or more computers act as servers. Servers provide a centralized repository of resources such as printers and files. Clients refer to a workstation that accesses the server."}
{"input": "What is Naive Bayes? Why is it Naive?", "output": "Naive Bayes classifiers are a series of classification algorithms that are based on the Bayes theorem. This family of algorithm shares a common principle which treats every pair of features independently while being classified. \n\nNaive Bayes is considered Naive because the attributes in it (for the class) is independent of others in the same class.  This lack of dependence between two attributes of the same class creates the quality of naiveness.\n"}
{"input": "What are some of the properties of a normal distribution?", "output": "A normal distribution, regardless of its size, will have a bell-shaped curve that is symmetric along the axes. Following are some of the important properties: Unimodal: It has only one mode. Symmetrical: Left and right halves of the curve are mirrored. Central tendency: The mean, median, and mode are at the midpoint."}
{"input": " List out some computer processors", "output": "Computer processors are: 1) Intel Core i9, 2) Intel Core i5, 3) Intel Core i7, 4) AMD Ryzen 7, and 5) AMD Ryzen 5."}
{"input": "What is the Six sigma in statistic?", "output": "In quality control, an error-free data set is generated using six sigma statistics. \u03c3 is known as standard deviation. The lower the standard deviation, the less likely that a process performs accurately and commits errors. If a process delivers 99.99966% error-free results, it is said to be six sigma. A six sigma model is one that outperforms 1\u03c3, 2\u03c3, 3\u03c3, 4\u03c3, and 5\u03c3 processes and is sufficiently reliable to deliver defect-free work."}
{"input": "Explain what is Radix Sort algorithm?", "output": "Encryption is the process of converting plaintext into a secret code format referred aRadix sort puts the element in order by comparing the digits of the numbers. It is one of the linear sorting algorithms for integers."}
{"input": "What is an artificial neural network?", "output": "An artificial neural network is inspired by the networks and functionalities of human biological neurons. it is also known as neural networks or neural nets. ANN uses layers of interconnected nodes called artificial neurons that work together to process and learn the input data. The starting layer artificial neural network is known as the input layer, it takes input from external input sources and transfers it to the next layer known as the hidden layer where each neuron received inputs from previous layer neurons and computes the weighted sum, and transfers to the next layer neurons. These connections are weighted means effects of the inputs from the previous layer are optimized more or less by assigning different-different weights to each input and it is adjusted during the training process by optimizing these weights for better performance of the model. The output of one neuron becomes the input to other neurons in the next layer of the network, and this process continues until the final layer produces the output of the network."}
{"input": " What is an algorithm? ", "output": "An algorithm is a step by step method of solving a problem or manipulating data. It defines a set of instructions to be executed in a certain order to get the desired output. "}
{"input": " What are loops?", "output": "The loop is a structure which can repeat a set of statements up to a fixed number of times or until a certain criterion is satisfied."}
{"input": " What is a linked list?", "output": "A linked list is a sequence of nodes in which each node is connected to the node following it. This forms a chain-like link for data storage."}
{"input": "What is DMA?", "output": "DMA, which stands for Direct Memory Access, is a feature of computer systems that allows an input/output device to receive or send data directly from or to the main memory, bypassing the CPU to boost memory operations. The process is performed by a chip known as the DMA controller."}
{"input": "What are the key assumptions of linear regression? How can they be checked?", "output": "Linear regression assumes linearity, independence of errors, homoscedasticity, normality of error distribution, and absence of multicollinearity. Linearity can be checked visually using scatter plots or quantitatively with a lack-of-fit test. Independence of errors is often assumed from study design but can also be tested using Durbin-Watson statistic for time series data. Homoscedasticity implies constant variance of errors across all levels of predictor variables. It can be assessed by plotting residuals against predicted values; ideally, this plot should show random scatter. Breusch-Pagan or White\u2019s tests are formal methods to check this assumption. Normality of error distribution can be examined through Q-Q plots or statistical tests like Shapiro-Wilk. The plot should closely follow the 45-degree reference line if errors are normally distributed. Multicollinearity refers to high correlation among predictor variables. Variance Inflation Factor (VIF) is commonly used to detect multicollinearity; VIF exceeding 5 indicates severe multicollinearity."}
{"input": " List the types of constructors", "output": "There are two types of the constructor: 1) parameterized constructor and 2) default constructor."}
{"input": " What are pointers?", "output": "Pointers point to specific areas in the memory. Pointers contain the address of a variable, which in turn may contain a value or even an address to another memory."}
{"input": " What is Cache?", "output": "A cache is a temporary storage location for frequently accessed data. Caches are useful for organizations that need to store large amounts of data in a short amount of time. They can also be used to improve performance by reducing the amount of data that needs to be retrieved from the network. Caches can be used to store data in several different ways. They can be used to store data that is frequently accessed, such as documents or images. They can also be used to store data that is rarely accessed, such as log files or application states. They can also be used to store data that is needed only occasionally, such as temporary files or cookies."}
{"input": "How do you interpret the results of a logistic regression analysis?", "output": "Logistic regression analysis results are interpreted through the odds ratio, which is derived from the coefficients of the predictor variables. The odds ratio represents the change in odds resulting from a one-unit change in the predictor while holding other predictors constant. A positive coefficient indicates an increase in the log-odds of the outcome, and thus increases the probability of the outcome occurring. Conversely, a negative coefficient decreases the probability. The significance of each predictor variable can be assessed using Wald\u2019s test. If the p-value is less than 0.05, we reject the null hypothesis that the coefficient equals zero, indicating the predictor significantly affects the outcome. Model fit can be evaluated using measures like Hosmer-Lemeshow goodness-of-fit test or area under the ROC curve (AUC). A well-fitting model has non-significant Hosmer-Lemeshow test and high AUC value close to 1."}
{"input": "How do you stay up-to-date with the new and upcoming concepts in statistics?", "output": "This is a commonly asked question in a statistics interview. Here, the interviewer is trying to assess your interest and ability to find out and learn new things efficiently. Do talk about how you plan to learn new concepts and make sure to elaborate on how you practically implemented them while learning."}
{"input": "Explain clustering support", "output": "Clustering support refers to the ability of a network operating system to connect multiple servers in a fault-tolerant group. The main purpose of this is the if one server fails, all processing will continue with the next server in the cluster."}
{"input": "What is the role of maximum likelihood in logistic regression.", "output": "Ans. Maximum likelihood equation helps in estimation of most probable values of the estimator\u2019s predictor variable coefficients which produces results which are the most likely or most probable and are quite close to the truth values.\n"}
{"input": "What is an Interface?", "output": "The interface is similar to a class in Java, but it is a collection of abstract methods. A class can have more than one interface."}
{"input": "Are there any risks to extrapolation? Explain with an example when you would and would not use this.", "output": "Extrapolation is essentially predicting values of the target function for parameter values outside the range of those observed during training. While extrapolation could reasonably work well in some cases, such as predicting the voltage in Ohm's law, it can also result in meaningless results. One easy example to explain this can be to extrapolate the decreasing rainfall trend at the end of the rainy season. If the extrapolation is done unchecked, the model could predict negative rain after a period that is about as nonsensical as it gets!"}
{"input": "What is a time series, and how is it different from cross-sectional data?", "output": "A time series is a sequence of observations recorded over time, usually at regular intervals. Time series data is different from cross-sectional data in that it tracks changes over time, while cross-sectional data is collected at a single point in time and typically captures information about a single population or sample."}
{"input": "Explain Markov's Decision Process.", "output": "A mathematical framework called the Markov Decision Process (MDP) is used to describe decision-making in circumstances where the result is partially determined by chance and partially controlled by the decision-maker. MDPs are widely used in the field of reinforcement learning as they provide a way to model an agent's decision-making problem.An MDP is defined by a set of states, a set of actions, a transition function that defines the probability of going from one state to another, a reward function that defines the immediate reward for being in a particular state and taking a particular action, and a discount factor that determines the importance of future rewards."}
{"input": "What are the assumptions made when building a linear regression model?", "output": "There is a linear relationship between the dependent variable (Y) and the independent variable (X). Homoscedasticity. The independent variables aren\u2019t highly correlated with each other (multicollinearity) The residuals follow a normal distribution and are independent of each other."}
{"input": "What is the importance of the OSI Physical Layer?", "output": "The physical layer does the conversion from data bits to the electrical signal, and vice versa. This is where network devices and cable types are considered and setup."}
{"input": "What is a Decoder?", "output": "The decoder is a type of circuit that converts the encoded data to its original format. It also converts the digital signal into an analog signal."}
{"input": "Briefly describe NAT", "output": "NAT is Network Address Translation. This is a protocol that provides a way for multiple computers on a common network to share a single connection to the Internet."}
{"input": " How machine learning is different from general programming?", "output": "In general programming, we have the data and the logic by using these two we create the answers. But in machine learning, we have the data and the answers and we let the machine learn the logic from them so, that the same logic can be used to answer the questions which will be faced in the future. Also, there are times when writing logic in codes is not possible so, at those times machine learning becomes a saviour and learns the logic itself."}
{"input": "What is a virtual memory?", "output": "Virtual memory is a memory management technique for letting processes execute outside of memory. This is very useful especially is an executing program cannot fit in the physical memory."}
{"input": "What is the Bernoulli distribution in Na\u00efve Bayes?", "output": "This is a distribution that evaluates a particular outcome as binary. For example, in the Bernoulli Na\u00efve Bayes classifier, given a word, that word can either be in a message or not."}
{"input": "Explain the difference between ridge and lasso regression.", "output": "Ridge regression or L2 regression, adds the sum of the square of weights as a penalty to the model\u2019s cost function. Lasso regression or L1 regression, adds absolute weights to the model\u2019s cost function. Lasso regression can also be used as a feature selection technique, as it can pull feature weights down to zero and eliminate variables that aren\u2019t necessary."}
{"input": "What do you mean by an operating system? What are its basic functions?", "output": "Operating System (OS) is basically a software program that manages and handles all resources of a computer such as hardware and software. The first OS was introduced in the early 1950s known as GMOs. An OS is responsible for managing, handling, and coordinating overall activities and sharing of computer resources. It acts as an intermediary among users of computer and computer hardware. "}
{"input": "What is Mean?", "output": "Mean is the average of a collection of values. We can calculate the mean by dividing the sum of all observations by the number of observations."}
{"input": "Explain One-hot encoding and Label Encoding. How do they affect the dimensionality of the given dataset?", "output": "One-hot encoding is the representation of categorical variables as binary vectors. Label Encoding is converting labels/words into numeric form. Using one-hot encoding increases the dimensionality of the data set. Label encoding doesn\u2019t affect the dimensionality of the data set. One-hot encoding creates a new variable for each level in the variable whereas, in Label encoding, the levels of a variable get encoded as 1 and 0.\n"}
{"input": "Your friend is training a logistic regression model for a binary classification problem using the L2 loss for optimization. Explain to him why this is a bad choice and which loss he should be using instead.", "output": "The L2 loss is suitable for a target, or a response variable that is continuous. On the other hand, in a binary classification problem using LR we would like the output to match either zero or one and a natural candidate for a loss function is the binary cross-entropy loss"}
{"input": " Define Cin and Cout in C++?", "output": "Cin and cout are two important functions in C++ that you should know about. Cin and cout are functions that allow you to read and write data to a stream."}
{"input": "Can you explain how Bayesian hierarchical models work?", "output": "Bayesian hierarchical models, also known as multilevel models, are statistical models that estimate parameters at multiple levels. They incorporate both individual-level and group-level information to make predictions. The model structure is divided into two parts: the likelihood function (data level) and prior distributions (parameter level). At the data level, observations are modeled based on parameters with uncertainty. At the parameter level, these uncertain parameters are themselves modeled, often using hyperparameters. This creates a hierarchy of parameters, hence the name. The Bayesian aspect comes from Bayes\u2019 theorem, which updates prior beliefs about parameters given observed data to provide posterior estimates. In this context, priors represent initial assumptions about parameters before observing data. Posterior distributions then combine these priors with the likelihood of observed data. This approach allows for more robust estimation in complex scenarios where traditional methods may fail. It can handle missing data, varying effects across groups, and other complexities by borrowing strength across different levels of hierarchy."}
{"input": " What is computer architecture?", "output": "Computer architecture refers to hardware instructions, software standards and technology infrastructure that define how computer platforms, systems and programs operate. This means that computer architecture outlines the system's functionality, design and compatibility."}
{"input": " Explain what is time complexity of Algorithm?", "output": "Time complexity of an algorithm indicates the total time needed by the program to run to completion. It is usually expressed by using the big O notation."}
{"input": "What are the maximum networks and hosts in class A, B, and C network?", "output": "For Class A, there are 126 possible networks and 16,777,214 hosts. For Class B, there are 16,384 possible networks and 65,534 hosts. For Class C, there are 2,097,152 possible networks and 254 hosts"}
{"input": "What are ARRAYs?", "output": "When dealing with arrays, data is stored and retrieved using an index that refers to the element number in the data sequence. This means that data can be accessed in any order. In programming, an array is declared as a variable having a number of indexed elements."}
{"input": "What are confounding variables?", "output": "A confounding variable is a factor that affects both the dependent and independent variables, making it seem like there is a causal relationship between them. For example, there is a high correlation between ice cream purchases and forest fires. The number of forest fires and ice cream sales increases at the same time. This is because the confounding variable between them is heat. As temperature rises, so does ice cream sales and the risk of forest fires."}
{"input": "State the case where the median is a better measure when compared to the mean.", "output": "In the case where there are a lot of outliers that can positively or negatively skew data, the median is preferred as it provides an accurate measure in this case of determination."}
{"input": "How do signed and unsigned numbers affect memory?", "output": "In the case of signed numbers, the first bit is used to indicate whether positive or negative, which leaves you with one bit short. With unsigned numbers, you have all bits available for that number. The effect is best seen in the number range (an unsigned 8-bit number has a range 0-255, while the 8-bit signed number has a range -128 to +127."}
{"input": "In NLP, The algorithm decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents a. Term Frequency (TF) b. Inverse Document Frequency (IDF) c. Word2Vec d. Latent Dirichlet Allocation (LDA)", "output": "Answer: b)\n"}
{"input": "What Role Does Computer Vision Play in AI?", "output": "Artificial intelligence (AI) is broken down into a number of subfields, one of which is known as computer vision. Computer vision is the process of teaching computers to understand and collect data from the visual environment, such as graphics. Therefore, AI technology is used by computer vision in order to address complicated challenges such as image analysis, object identification, and other similar issues."}
{"input": "An argument erupts between two senior data-scientists regarding the choice of an approach for training of a very small medical corpus. One suggest that bagging is superior while the other suggests stacking. Which technique, bagging or stacking, in your opinion is superior? Explain in detail. (i) Stacking since each classier is trained on all of the available data. (ii) Bagging since we can combine as many classifiers as we want by training each on a different sub-set of the training corpus.", "output": "The correct choice would be stacking. In cases where the given corpus is small, we would most likely prefer training our models on the full data-set. "}
{"input": "How to convert normal distribution to standard normal distribution?", "output": "Any point (x) from the normal distribution can be converted into standard normal distribution (Z) using this formula \u2013 Z(standardized) = (x-\u00b5) / \u03c3. Here, Z for any particular x value indicates how many standard deviations x is away from the mean of all values of x."}
{"input": "What is IPv6?", "output": "Pv6, or Internet Protocol version 6, was developed to replace IPv4. At present, IPv4 is being used to control internet traffic but is expected to get saturated in the near future. IPv6 was designed to overcome this limitation."}
{"input": " What is MESI?", "output": "MESI stands for the four states of the cache blocks, which are Modified, Exclusive, Shared and Invalid. It's also known as the 'Illinois protocol'. It's used to maintain cache coherency in hierarchical memory systems. MESI is the most common protocol that supports write-back cache. Its use in personal computers became common with the introduction of Intel's Pentium processor."}
{"input": "Is Overfitting a possibility with linear regression?", "output": "Yes, Overfitting is possible even with linear regression. This happens when multiple linear regression is used to fit an extremely high-degree polynomial. When the parameters of such a model are learned, they will fit too closely to the training data, fitting even the noise, and thereby fail to generalize on test data."}
{"input": "Define likelihood and evidence in Naive Bayes?", "output": "The probability of event B given that event A is true is called likelihood and the independent probability of event B is called evidence.  P(B|A) = likelihood, P(B) = evidence"}
{"input": "What are the different layers of a convoluted neural network?", "output": "The different types of layers of a CNN include:\n\nConvolutional Layer: This is the core layer which has sets of learnable filters with receptive field. This is the first layer which extracts features from the input data.\n\nReLU Layer: This layer converts negative pixels to zero by making the networks non-linear.\n\nPooling Layer: This layer progressively reduces the spatial size of the representation by reducing computation and parameters in the network. The most common approach of pooling is max pooling.\n"}
{"input": "What is the law of large numbers in statistics?", "output": "The law of large numbers in statistics is a theory that states that the increase in the number of trials performed will cause a positive proportional increase in the average of the results becoming the expected value. Example: The probability of flipping a fair coin and landing heads is closer to 0.5 when it is flipped 100,000 times when compared to 100 flips."}
{"input": "State the limitations of Fixed Basis Function.", "output": "Linear separability in feature space doesn\u2019t imply linear separability in input space. So, Inputs are non-linearly transformed using vectors of basic functions with increased dimensionality. Limitations of Fixed basis functions are:\n\nNon-Linear transformations cannot remove overlap between two classes but they can increase overlap.\n\nOften it is not clear which basis functions are the best fit for a given task. So, learning the basic functions can be useful over using fixed basis functions.\n\nIf we want to use only fixed ones, we can use a lot of them and let the model figure out the best fit but that would lead to overfitting the model thereby making it unstable. \n"}
{"input": "You created a document term matrix on the input data of 20K documents for a Machine learning model. Which of the following can be used to reduce the dimensions of data? 1. Keyword Normalization 2. Latent Semantic Indexing 3. Latent Dirichlet Allocation a. only 1 b. 2, 3 c. 1, 3 d. 1, 2, 3 Choose a., b., c., or d.", "output": "Answer: d)\n"}
{"input": "Explain what a \u201cHash Algorithm\u201d is and what are they used for?", "output": "Hash Algorithm\u201d is a hash function that takes a string of any length and decreases it to a unique fixed length string. It is used for password validity, message & data integrity and for many other cryptographic systems."}
{"input": ".Explain demand paging?", "output": "Demand paging is a method that loads pages into memory on demand. This method is mostly used in virtual memory. In this, a page is only brought into memory when a location on that particular page is referenced during execution."}
{"input": "What is an interaction term in linear regression and how is it used?", "output": "An interaction term in linear regression is a term in the regression model that represents the effect of the interaction between two or more variables on the dependent variable. It is used to evaluate the combined effect of multiple variables on the dependent variable, and to identify non-linear relationships between the variables. In a regression model with an interaction term, the effect of one variable on the dependent variable may be different at different levels of the other variable. This allows for a more nuanced understanding of the relationships between the variables in the model."}
{"input": "When a logical error occurs?", "output": "The logical error happens when a program implements the wrong logic. The translator (compiler or interpreter) does not report any error message for a logical error. These errors are the most difficult to locate."}
{"input": "What is the best dataset scenario for the Na\u00efve Bayes Classifier?", "output": "If the training data is smaller or if the dataset has fewer number of observations (samples) and a high number of features. Na\u00efve Bayes works well on this data because of its High bias \u2013 Low variance trade off."}
{"input": "What is Random Sampling? Give some examples of some random sampling techniques.", "output": "Random sampling is a sampling method in which each sample has an equal probability of being chosen as a sample. It is also known as probability sampling. Let us check four main types of random sampling techniques \u2013 Simple Random Sampling technique \u2013 In this technique, a sample is chosen randomly using randomly generated numbers. A sampling frame with the list of members of a population is required, which is denoted by \u2018n\u2019. Using Excel, one can randomly generate a number for each element that is required. Systematic Random Sampling technique -This technique is very common and easy to use in statistics. In this technique, every k\u2019th element is sampled. For instance, one element is taken from the sample and then the next while skipping the pre-defined amount or \u2018n\u2019. In a sampling frame, divide the size of the frame N by the sample size (n) to get \u2018k\u2019, the index number. Then pick every k\u2019th element to create your sample. Cluster Random Sampling technique -In this technique, the population is divided into clusters or groups in such a way that each cluster represents the population. After that, you can randomly select clusters to sample. Stratified Random Sampling technique \u2013 In this technique, the population is divided into groups that have similar characteristics. Then a random sample can be taken from each group to ensure that different segments are represented equally within a population. "}
{"input": "What is RIP?", "output": "RIP, short for Routing Information Protocol is used by routers to send data from one network to another. It efficiently manages routing data by broadcasting its routing table to all other routers within the network. It determines the network distance in units of hops."}
{"input": "Is it possible to test for the probability of improving model accuracy without cross-validation techniques? If yes, please explain.", "output": "Yes, it is possible to test for the probability of improving model accuracy without cross-validation techniques. We can do so by running the ML model for say n number of iterations, recording the accuracy. Plot all the accuracies and remove the 5% of low probability values. Measure the left [low] cut off and right [high] cut off. With the remaining 95% confidence, we can say that the model can go as low or as high [as mentioned within cut off points]. \n"}
{"input": "If your dataset is suffering from high variance, how would you handle it?", "output": "For datasets with high variance, we could use the bagging algorithm to handle it. Bagging algorithm splits the data into subgroups with sampling replicated from random data. After the data is split, random data is used to create rules using a training algorithm. Then we use polling technique to combine all the predicted outcomes of the model.\n"}
{"input": "True or False: Unlike CNN architectures such as AlexNet or VGG, ResNet does not have any hidden FC layers.", "output": "True. The ResNet architecture terminates with a global average pooling layer followed by a K-way FC layer with a softmax activation function, where K is the number of classes (ImageNet has 1000 classes). Therefore, the ResNet has no hidden FC layers."}
{"input": "What is DOE?", "output": "DOE is an acronym for the Design of Experiments in statistics. It is considered as the design of a task that describes the information and the change of the same based on the changes to the independent input variables."}
{"input": "Is it always necessary to use an 80:20 ratio for the train test split?", "output": "No there is no such necessary condition that the data must be split into 80:20 ratio. The main purpose of the splitting is to have some data which the model has not seen previously so, that we can evaluate the performance of the model. If the dataset contains let\u2019s say 50,000 rows of data then only 1000 or maybe 2000 rows of data is enough to evaluate the model\u2019s performance."}
{"input": " What are the operations that can be performed on a stack? ", "output": "A stack is a linear data structure that operates on the same concept, in that components in a stack are added and deleted only from one end, referred to as the TOP. As a result, a stack is known as a LIFO (Last-In-First-Out) data structure because the piece that was put last is the first to be removed"}
{"input": " Explain class variable", "output": "Variables represent a memory of class, which it shares with each and every instance."}
{"input": "What is Gradient Clipping?", "output": "Gradient clipping is a technique used to prevent the exploding gradient problem during the training of deep neural networks. It involves rescaling the gradient when its norm exceeds a certain threshold. The idea is to clip the gradient, i.e., set a maximum value for the norm of the gradient, so that it does not become too large during the training process. This technique ensures that the gradients don\u2019t become too large and prevent the model from diverging. Gradient clipping is commonly used in recurrent neural networks (RNNs) to prevent the exploding gradient problem."}
{"input": "In information theory, minus the logarithm of the probability of a symbol (essentially the number of bits required to represent it efficiently in a binary code) is defined to be the information conveyed by transmitting that symbol. In this context, the entropy can be interpreted as the expected information conveyed by transmitting a single symbol from an alphabet in which the symbols occur with the probabilities \u03c0_k. Mark the correct answer: Information is a/an [decrease/increase] in uncertainty", "output": "Information is an [increase] in uncertainty"}
{"input": "Why are generative adversarial networks (GANs) so popular?", "output": "Generative adversarial networks are used for a variety of purposes. In the case of working with images, they have a high amount of traction and efficient working. Creation of art: GANs are used to create artistic images, sketches, and paintings. Image enhancement: They are used to greatly enhance the resolution of the input images. Image translation: They are also used to change certain aspects, such as day to night and summer to winter, in images easily."}
{"input": " What is the Difference Between Parametric and Non-parametric Models?", "output": "In statistics and machine learning, a parametric model is a model that has a fixed number of parameters. These parameters have specific meanings and can be estimated from the data using a method such as maximum likelihood estimation. Once the parameters are estimated, the model can be used to make predictions or estimate the probability of certain events."}
{"input": "Which norm does the following equation represent? |x1 - x2| + |y1 - y2|", "output": "The L2 norm."}
{"input": "Is it necessary to remove outliers? Why or why not?", "output": "Yes, it is necessary to remove outliers as they can have a huge impact on the model's predictions. Take, for instance, plots 3 and 4 for the Anscombe's quartet provided above. It is apparent from these plots that the outliers have caused a significant change in the best fit line in comparison to what it would have been in their absence."}
{"input": "What is the Binomial Distribution Formula?", "output": "The binomial distribution formula is: b(x; n, P) = nCx * Px * (1 \u2013 P)n \u2013 x. Where: b = binomial probability. x = total number of \u201csuccesses\u201d (pass or fail, heads or tails, etc.). P = probability of success on an individual trial. n = number of trials"}
{"input": "  Why is it that not all header files are declared in every C program?", "output": "The choice of declaring a header file at the top of each C program would depend on what commands/functions you will be using in that program. Since each header file contains different function definitions and prototype, you would be using only those header files that would contain the functions you will need. Declaring all header files in every program would only increase the overall file size and load of the program, and is not considered a good programming style."}
{"input": "What is a VPN?", "output": "VPN means Virtual Private Network, a technology that allows a secure tunnel to be created across a network such as the Internet. For example, VPNs allow you to establish a secure dial-up connection to a remote server."}
{"input": "Assume a source of signals that transmits a given message a with probability Pa. Assume further that the message is encoded into an ordered series of ones and zeros (a bit string) and that a receiver has a decoder that converts the bit string back into its respective message. Shannon devised a formulae that describes the size that the mean length of the bit string can be compressed to. Write the formulae.", "output": "This quantity ISh, represented in the formulae is called the Shannon information of the source: I_{Sh} = \u2212sum_a p_a log_2 p_a. It refers to the mean length in bits, per message, into which the messages can be compressed to. It is then possible for a communications channel to transmit ISh bits per message with a capacity of I_{Sh}. "}
{"input": "What are stop words?", "output": "Stop words are said to be useless data for a search engine. Words such as articles, prepositions, etc. are considered stop words. There are stop words such as was, were, is, am, the, a, an, how, why, and many more. In Natural Language Processing, we eliminate the stop words to understand and analyze the meaning of a sentence. The removal of stop words is one of the most important tasks for search engines. Engineers design the algorithms of search engines in such a way that they ignore the use of stop words. This helps show the relevant search result for a query."}
{"input": "How do you handle missing values in time series data?", "output": "In time series data, handling missing values is crucial. There are several methods to do this: 1. Forward Fill: This method propagates the last observed value forward until a new value is encountered. 2. Backward Fill: It works in reverse direction of \u2018forward fill\u2019, filling future observations with the latest past observation. 3. Linear Interpolation: Here, we draw a line between two points and pick a point on that line for the missing value. 4. Mean Substitution: Replace missing values with the mean of available data. 5. Regression: We can use regression models where missing values are treated as the dependent variable. The choice of method depends on the nature of the data and the specific requirements of the analysis."}
{"input": "Which performance metric is better R2 or adjusted R2?", "output": "Ans. Adjusted R2 because the performance of predictors impacts it. R2 is independent of predictors and shows performance improvement through increase if the number of predictors is increased.\n"}
{"input": "Data Science, which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge. The data can be either coded or uncoerced. Data mining and data science are similar because both provide abstract data from a large amount of data. Data science now includes computer science, mathematical statistics, and computer science and behavioural applications. Data science, which integrates data analysis, understanding, organization, and communication together, produces insights and knowledge from a large amount of data by combining statistical analysis, visualization, and applied mathematical economics. The collection, analysis, interpretation, organization, and presentation of data are the main components of data science.", "output": ""}
{"input": " Explain pros and cons of a command line interface?", "output": "A command line interface allows the user to type in commands that can immediately provide results. Many seasoned computer users are well accustomed to using the command line because they find it quicker and simpler.However, the main problem with a command line interface is that users have to be familiar with the commands, including the switches and parameters that come with it. This is a downside for people who are not fond of memorizing commands.."}
{"input": "Can you provide an example of how you would use ANOVA in a real-world data analysis project?", "output": "Sure! Let\u2019s consider a scenario where a pharmaceutical company is testing the efficacy of three different drugs (Drug A, Drug B, and Drug C) to treat a certain medical condition. The dependent variable is the improvement in the patient\u2019s symptoms, and the independent variable is the drug administered. We collect data from randomized controlled trials for each drug and conduct a one-way ANOVA test to determine if there are significant differences in the mean improvement scores between the three drugs. If the test is significant, we can follow up with post-hoc tests, such as Tukey\u2019s Honestly Significant Difference (HSD) test, to identify which drug(s) show significantly better outcomes compared to others."}
{"input": "What are inferential statistics used for?", "output": "In inferential statistics, we use some sample data to draw conclusions about a population. From government operations to quality control and quality assurance teams in multinational corporations, inferential statistics are used in a variety of fields."}
{"input": " What is dynamic data structure?", "output": "Dynamic data structure provides a means for storing data more efficiently into memory. Using Using dynamic memory allocation, your program will access memory spaces as needed. This is in contrast to static data structure, wherein the programmer has to indicate a fix number of memory space to be used in the program."}
{"input": "How would you choose the right prior when working with Bayesian Inference?", "output": "Choosing the right prior in Bayesian Inference depends on several factors. If there\u2019s substantial knowledge about the problem, an informative prior can be used to incorporate this information into the model. This could be a probability distribution reflecting known or estimated values. However, if little is known, non-informative priors are preferred. These have minimal impact on posterior distributions and allow data to speak more strongly. Common choices include uniform or Jeffreys\u2019 priors. In some cases, conjugate priors are chosen for mathematical convenience as they yield posterior distributions of the same family as the prior. Lastly, empirical Bayes methods use data to estimate the prior. While not fully Bayesian, it provides computational advantages in complex problems. The choice ultimately depends on available knowledge, computational feasibility, and the specific context of the problem at hand."}
{"input": "Can you talk about the concept of cointegration and its significance in time series data?", "output": "Cointegration is a statistical property of time series variables. If two or more series are individually non-stationary, but a linear combination of them is stationary, they are said to be cointegrated. Cointegration has significant implications in the modeling and forecasting of time series data. It allows us to predict long-term equilibrium relationships among non-stationary variables, which can\u2019t be detected through traditional correlation analysis. This concept is crucial in finance for portfolio optimization where assets prices move together over time despite being individually random. In econometrics, it\u2019s used to establish stable, long-run relationships between economic variables like income and consumption."}
{"input": "Mention why feature engineering is important in model building and list out some of the techniques used for feature engineering.", "output": "Algorithms necessitate features with some specific characteristics to work appropriately. The data is initially in a raw form. You need to extract features from this data before supplying it to the algorithm. This process is called feature engineering. When you have relevant features, the complexity of the algorithms reduces. Then, even if a non-ideal algorithm is used, results come out to be accurate.\n\nFeature engineering primarily has two goals:\n\nPrepare the suitable input data set to be compatible with the machine learning algorithm constraints.\n\nEnhance the performance of machine learning models.\n\nSome of the techniques used for feature engineering include Imputation, Binning, Outliers Handling, Log transform, grouping operations, One-Hot encoding, Feature split, Scaling, Extracting date.\n"}
{"input": "What is a chi-square test?", "output": "Ans. A chi-square determines if a sample data matches a population. \n\nA chi-square test for independence compares two variables in a contingency table to see if they are related.\n\nA very small chi-square test statistics implies observed data fits the expected data extremely well. \n"}
{"input": "If we initialize the weights of a tanh based NN, which of the following approaches will lead to the vanishing gradients problem?. (i) Using the normal distribution, with parameter initialization method as suggested by Kaiming. (ii) Using the uniform distribution, with parameter initialization method as suggested by Xavier Glorot. (iii) Initialize all parameters to a constant zero value.", "output": "(iii) Initialize all parameters to a constant zero value. When we apply the tanh function to an input which is very large, the output which is almost zero, will be propagated to the remaining partial derivatives leading to the well known phenomenon."}
{"input": "What are the types of selection bias in statistics?", "output": "There are many types of selection bias as shown below: Observer selection, Attrition, Protopathic bias, Time intervals, Sampling bias"}
{"input": "Explain the meaning of file.", "output": "A file is a named location that stores information or data permanently. It is always stored in the storage device using a file name with primary and secondary name, which is separated by a \u201c.\u201d(DOT)."}
{"input": " How would a file name EXAMPLEFILE.TXT appear when viewed under the DOS command console operating in Windows 98?", "output": "The filename would appear as EXAMPL~1.TXT . The reason behind this is that filenames under this operating system are limited to 8 characters when working under DOS environment."}
{"input": "What are some common methods used for time series forecasting?", "output": "Some common methods used for time series forecasting include ARIMA models, exponential smoothing, and machine learning algorithms such as random forests and neural networks."}
{"input": "Define stride in CNN.", "output": "Stride is the number of pixels or units that a kernel is moved across the input data while performing convolution operations in Convolutional Neural Networks (CNNs). It is one of the hyperparameters of a CNN that can be manipulated to control the output feature map\u2019s size. During the forward pass, we slide each filter through the entire input image matrix step by step, where each step is known as stride (which can have a value of 2, 3, or even 4 for high-dimensional images), and we compute the dot product between the kernel weights and patch from input volume."}
{"input": "  What are the layers of the OSI reference model?", "output": "A backbone network is a centralized infrastructure that is designed to distribute different routes and data to various networks. It also handles the management of bandwidth and multiple channels."}
{"input": "What is a bell-curve distribution?", "output": "A bell-curve distribution is represented by the shape of a bell and indicates normal distribution. It occurs naturally in many situations especially while analyzing financial data. The top of the curve shows the mode, mean and median of the data and is perfectly symmetrical. The key characteristics of a bell-shaped curve are \u2013 The empirical rule says that approximately 68% of data lies within one standard deviation of the mean in either of the directions. Around 95% of data falls within two standard deviations and Around 99.7% of data fall within three standard deviations in either direction. "}
{"input": " Describe the objective of multiprogramming.", "output": "The main objective of multiprogramming is to have a process running at all times. With this design, CPU utilization is said to be maximized."}
{"input": "When a runtime error occurs?", "output": "A runtime error occurs when the computer is directed to perform an illegal operation by the program such as dividing a number by zero. Runtime errors are the only errors which are displayed immediately during the execution of a program. When these errors occur, the computer stops the execution of the programming and can display a diagnostic message that will help in locating the error."}
{"input": "What protocols fall under the TCP/IP Internet Layer?", "output": "There are 4 protocols that are being managed by this layer. These are ICMP, IGMP, IP, and ARP."}
{"input": "What are the criteria that Binomial distributions must meet?", "output": "Here are the three main criteria that Binomial distributions must meet \u2013The number of observation trials must be fixed. It means that one can only find the probability of something when done only a certain number of times. Each trial needs to be independent. It means that none of the trials should impact the probability of other trials. The probability of success remains the same across all trials. "}
{"input": "What is a computational graph in Deep Learning? ", "output": "A computation graph is a series of operations that are performed to take inputs and arrange them as nodes in a graph structure. It can be considered as a way of implementing mathematical calculations into a graph. This helps in parallel processing and provides high performance in terms of computational capability."}
{"input": "What are the main measures of central tendency in Descriptive Statistics?", "output": "The main measures of central tendency are the mean, median, and mode. The mean is the sum of all values divided by the total number of observations and is sensitive to outliers. The median is the middle value of a dataset when it\u2019s arranged in ascending or descending order and is more robust to outliers. The mode is the value that appears most frequently in the dataset."}
{"input": "Where Does Artificial Intelligence Go From Here?", "output": "This is one of the next important AI questions. It is anticipated that artificial intelligence will continue to have a significant impact on a large number of people as well as almost every sector. Artificial intelligence has become the primary impetus behind the development of new technologies such as robots, the Internet of Things, and large data sets. AI is capable of making an ideal judgment in a split second, which is almost difficult for a person to do. Cancer treatment, cutting-edge global climate solutions, smart transportation, and space research are all being aided by AI. We don't expect it to renounce its position as the driving force behind computer innovation and progress any time soon. Artificial Intelligence will have a greater influence on the globe than any other technological advancement in human history."}
{"input": "List the advantages and limitations of the Temporal Difference Learning Method.", "output": "Temporal Difference Learning Method is a mix of Monte Carlo method and Dynamic programming method. Some of the advantages of this method include:\n\nIt can learn in every step online or offline.\n\nIt can learn from a sequence which is not complete as well.\n\nIt can work in continuous environments.\n\nIt has lower variance compared to MC method and is more efficient than MC method.\n\nLimitations of TD method are:\n\nIt is a biased estimation.\n\nIt is more sensitive to initialization.\n"}
{"input": "What is KNN Imputer? ", "output": "We generally impute null values by the descriptive statistical measures of the data like mean, mode, or median but KNN Imputer is a more sophisticated method to fill the null values. A distance parameter is also used in this method which is also known as the k parameter. The work is somehow similar to the clustering algorithm. The missing value is imputed in reference to the neighborhood points of the missing values. "}
{"input": "What are descriptive statistics?", "output": "Descriptive statistics are used to summarize the basic characteristics of a data set in a study or experiment. It has three main types \u2013 Distribution \u2013 refers to the frequencies of responses. Central Tendency \u2013 gives a measure or the average of each response. Variability \u2013 shows the dispersion of a data set."}
{"input": "What is Q-Learning?", "output": "Q-learning is a type of reinforcement learning algorithm that is used to find the optimal policy for an agent to follow in an environment. The goal of Q-learning is to learn a function, called the Q-function, that maps states of the environment to the expected cumulative reward of taking a specific action in that state and then following the optimal policy afterwards.The Q-function is represented as a table, with each entry representing the expected cumulative reward of taking a specific action in a specific state. The Q-learning algorithm updates the Q-function by using the Bellman equation, which states that the value of the Q-function for a given state and action is equal to the immediate reward for taking that action in that state, plus the maximum expected cumulative reward of the next state."}
{"input": "What is the difference between an error of type I and an error of type II?", "output": "When the null hypothesis is rejected even though it is correct, a type 1 error occurs. False positives are also known as type 1 errors. When the null hypothesis is not rejected despite being incorrect, a type 2 error occurs. This is also known as a false negative."}
{"input": "Is it necessary to visualize the data when you have fitted a line? Why or why not?", "output": "It is crucial to visualize the data when you fit a line because numerically fitting a line is easy with pure numerical analysis or methods like Least Squares regression. But determining whether these fitted lines make any sense requires further analysis of which visualizing the data is one of the easiest ways. To underscore this point, let us consider Anscombe's quartet, the four datasets with almost identical simple statistical properties but appear wildly different in distribution when plotted. These four datasets result in a regression line with a slope of 0.50 and an intercept of 3.00. However, on plotting this line with the datasets, it becomes apparent that although fitted for datasets with almost identical results through quantitative analysis, the resultant line does not make any sense."}
{"input": "Which of the text parsing techniques can be used for noun phrase detection, verb phrase detection, subject detection, and object detection in NLP. a. Part of speech tagging b. Skip Gram and N-Gram extraction c. Continuous Bag of Words d. Dependency Parsing and Constituency Parsing", "output": "Answer: d)\n"}
{"input": "What is the concept of autocorrelation? How do you detect and correct it?", "output": "Autocorrelation, also known as serial correlation, refers to the degree of similarity between a given time series and a lagged version of itself over successive time intervals. It\u2019s used to detect non-randomness in data. Detecting autocorrelation can be done using graphical methods like scatter plots or by statistical tests such as Durbin-Watson test. Scatter plots visually show if there is any pattern while Durbin-Watson test provides a statistic that ranges from 0 (positive autocorrelation) to 4 (negative autocorrelation), with around 2 indicating no autocorrelation. Correcting autocorrelation involves transforming the data to make it stationary, meaning its properties do not depend on the time at which the series is observed. This could involve differencing the data, logarithmic transformation, or deflating the series if it\u2019s monetary. Another method is using autoregressive models where current values are regressed on past values."}
{"input": "Can you explain the key difference between Time Series data and cross-sectional data?", "output": "Time Series data and Cross-sectional data differ primarily in their structure. Time Series data is a collection of observations obtained sequentially over time, like stock prices or weather patterns. It\u2019s characterized by its temporal order, meaning the sequence matters. This allows for analysis of trends, seasonality, and forecasting. Cross-sectional data, on the other hand, is collected at a single point in time across multiple subjects, such as a population census. The focus here is comparison between different entities rather than tracking changes over time. The key difference lies in the dimension of interest: Time Series focuses on change over time within a single entity, while cross-sectional data emphasizes differences among entities at a specific moment."}
{"input": "What is a vanishing gradient when using RNNs? ", "output": "Vanishing gradient is a scenario that occurs when we use RNNs. Since RNNs make use of backpropagation, gradients at every step of the way will tend to get smaller as the network traverses through backward iterations. This equates to the model learning very slowly, thereby, causing efficiency problems in the network."}
{"input": "What is multicollinearity? How does it impact the performance of a regression model?", "output": "Multicollinearity occurs when two or more independent variables in the dataset are highly correlated with each other. In a regression model, multicollinearity can harm the interpretability of the model because it will be difficult to distinguish the individual effects of each variable on the model."}
{"input": "What is information extraction?", "output": "Information extraction in the context of Natural Language Processing refers to the technique of extracting structured information automatically from unstructured sources to ascribe meaning to it. This can include extracting information regarding attributes of entities, relationship between different entities and more. The various models of information extraction includes:\n\nTagger Module\n\nRelation Extraction Module\n\nFact Extraction Module\n\nEntity Extraction Module\n\nSentiment Analysis Module\n\nNetwork Graph Module\n\nDocument Classification & Language Modeling Module\n"}
{"input": "What is Lists in Python?", "output": "Python Tutorial for Beginners | Python Programming | Learn Python | Great Learning\n\nLists is an effective data structure provided in python. There are various functionalities associated with the same. Let us consider the scenario where we want to copy a list to another list. If the same operation had to be done in C programming language, we would have to write our own function to implement the same.\n\nOn the contrary, Python provides us with a function called copy. We can copy a list to another just by calling the copy function.\n\nnew_list = old_list.copy()\n\nWe need to be careful while using the function. copy() is a shallow copy function, that is, it only stores the references of the original list in the new list. If the given argument is a compound data structure like a list then python creates another object of the same type (in this case, a new list) but for everything inside old list, only their reference is copied. Essentially, the new list consists of references to the elements of the older list.\n\nHence, upon changing the original list, the new list values also change. This can be dangerous in many applications. Therefore, Python provides us with another functionality called as deepcopy.  Intuitively, we may consider that deepcopy() would follow the same paradigm, and the only difference would be that for each element we will recursively call deepcopy. Practically, this is not the case.\n\ndeepcopy() preserves the graphical structure of the original compound data. Let us understand this better with the help of an example:\n\nimport copy.deepcopy\n\na = [1,2]\n\nb = [a,a] # there's only 1 object a\n\nc = deepcopy(b)\n\n# check the result by executing these lines\n\nc[0] is a # return False, a new object a' is created\n\nc[0] is c[1] # return True, c is [a',a'] not [a',a'']\n\nThis is the tricky part, during the process of deepcopy() a hashtable implemented as a dictionary in python is used to map: old_object reference onto new_object reference. \n\nTherefore, this prevents unnecessary duplicates and thus preserves the structure of the copied compound data structure. Thus, in this case, c[0] is not equal to a, as internally their addresses are different.\n\nNormal copy\n\n>>> a = [[1, 2, 3], [4, 5, 6]]\n\n>>> b = list(a)\n\n>>> a\n\n[[1, 2, 3], [4, 5, 6]]\n\n>>> b\n\n[[1, 2, 3], [4, 5, 6]]\n\n>>> a[0][1] = 10\n\n>>> a\n\n[[1, 10, 3], [4, 5, 6]]\n\n>>> b   # b changes too -> Not a deepcopy.\n\n[[1, 10, 3], [4, 5, 6]]\n\nDeep copy\n\n>>> import copy\n\n>>> b = copy.deepcopy(a)\n\n>>> a\n\n[[1, 10, 3], [4, 5, 6]]\n\n>>> b\n\n[[1, 10, 3], [4, 5, 6]]\n\n>>> a[0][1] = 9\n\n>>> a\n\n[[1, 9, 3], [4, 5, 6]]\n\n>>> b    # b doesn't change -> Deep Copy\n\n[[1, 10, 3], [4, 5, 6]]\n\nNow that we have understood the concept of lists, let us solve interview questions to get better exposure on the same.\n"}
{"input": "What are some of the low and high-bias Machine Learning algorithms?", "output": "There are many low and high-bias Machine Learning algorithms, and the following are some of the widely used ones: Low bias: SVM, decision trees, KNN algorithm, etc. High bias: Linear and logistic regression"}
{"input": "What ensemble technique is used by Random forests?", "output": "Ans. Bagging is the technique used by Random Forests. Random forests are a collection of trees which work on sampled data from the original dataset with the final prediction being a voted average of all trees.\n"}
{"input": "What is Integrated Development Environment?", "output": "An IDE is a GUI-based software program. It is designed to help programmers build applications with all the needed programs and libraries."}
{"input": "While calculating the probability of a given situation, what error can we run into in Na\u00efve Bayes and how can we solve it?", "output": "We might encounter the zero division error when the probability for a particular scenario in the numerator is zero. To mitigate this, we can use Laplace Smoothing which basically adds a number to the numerator and another number to the denominator."}
{"input": "What are dynamic data structures?", "output": "Dynamic data structures are structures that expand and contract as a program runs. It provides a flexible means of manipulating data because it can adjust according to the size of the data."}
{"input": "What are the different kinds of variables or levels of measurement?", "output": "A variable can be categorized as one of four types: Ordinal, Interval, Ratio, or Nominal. Scale and Continuous are sometimes used to describe Interval and Ratio levels of measurement, respectively."}
{"input": "Word embeddings capture multiple dimensions of data and are represented as vectors a. True b. False", "output": "Answer: a)\n"}
{"input": "What are the assumptions of the t-test, and how do you check for them?", "output": "The assumptions of the t-test are: 1) The data within each group follows a normal distribution, 2) The variances of the groups are equal (homoscedasticity), and 3) The observations are independent. We can check for normality using visual methods like histograms or statistical tests like the Shapiro-Wilk test. To assess equal variances, we can use tests like Levene\u2019s test or conduct visual inspections, such as scatterplots or boxplots."}
{"input": "Does a symmetric distribution need to be unimodal?", "output": "A symmetric distribution does not need to be unimodal (having only one mode or one value that occurs most frequently). It can be bi-modal (having two values that have the highest frequencies) or multi-modal (having multiple or more than two values that have the highest frequencies)."}
{"input": "True or false: Bagging assumes that if the single predictors have independent errors, then a majority vote of their outputs should be better than the individual predictions.", "output": "True. The total error decreases with the addition of predictors to the ensemble."}
{"input": "Describe the importance of cross-validation in statistical analysis.", "output": "Cross-validation is a vital statistical tool used to assess the predictive performance of models and prevent overfitting. It involves partitioning data into subsets, training the model on one subset (training set), and validating it on another (validation set). This process helps in estimating how accurately a predictive model will perform in practice. The main types include k-fold, stratified, and leave-one-out cross-validation. K-fold divides data into \u2018k\u2019 subsets, repeating the train-validate process \u2018k\u2019 times. Stratified ensures each fold has the same proportion of observations with a given categorical outcome. Leave-one-out uses a single observation as validation set. Cross-validation provides an unbiased evaluation of model generalization on unseen data, enhancing its reliability."}
{"input": "Given an array of integers where each element represents the max number of steps that can be made forward from that element. The task is to find the minimum number of jumps to reach the end of the array (starting from the first element). If an element is 0, then cannot move through that element.", "output": "Solution: This problem is famously called as end of array problem. We want to determine the minimum number of jumps required in order to reach the end. The element in the array represents the maximum number of jumps that, that particular element can take.\n\nLet us understand how to approach the problem initially. \n\nWe need to reach the end. Therefore, let us have a count that tells us how near we are to the end. Consider the array A=[1,2,3,1,1]\n\nIn the above example we can go from \n\n> 2 - >3 - > 1 - > 1 - 4 jumps\n\n1 - > 2 - > 1 - > 1 - 3 jumps\n\n1 - > 2 - > 3 - > 1 - 3 jumps\n\nHence, we have a fair idea of the problem. Let us come up with a logic for the same. \n\nLet us start from the end and move backwards as that makes more sense intuitionally. We will use variables right and prev_r denoting previous right to keep track of the jumps. \n\nInitially, right = prev_r = the last but one element. We consider the distance of an element to the end, and the number of jumps possible by that element. Therefore, if the sum of the number of jumps possible and the distance is greater than the previous element, then we will discard the previous element and use the second element\u2019s value to jump. Try it out using a pen and paper first. The logic will seem very straight forward to implement. Later, implement it on your own and then verify with the result.\n\ndef min_jmp(arr):\n\n    n = len(arr)\n\n    right = prev_r = n-1\n\n    count = 0\n\n    # We start from rightmost index and travesre array to find the leftmost index\n\n    # from which we can reach index 'right'\n\n    while True:\n\n        for j in (range(prev_r-1,-1,-1)):\n\n            if j + arr[j] >= prev_r:\n\n                right = j\n\n        if prev_r != right:\n\n            prev_r = right\n\n        else:\n\n            break\n\n        count += 1\n\n    return count if right == 0 else -1\n\n# Enter the elements separated by a space\n\narr = list(map(int, input().split()))\n\nprint(min_jmp(n, arr))\n"}
{"input": "What do you mean by process synchronization?", "output": "Process synchronization is basically a way to coordinate processes that use shared resources or data. It is very much essential to ensure synchronized execution of cooperating processes so that will maintain data consistency. Its main purpose is to share resources without any interference using mutual exclusion."}
{"input": "Briefly describe how neural style transfer (NST) works.", "output": "The algorithm suggests how to combine the content a first image with the style of a second image to generate a third, stylized image using CNNs."}
{"input": "What is the difference between regularization and normalisation?", "output": "Normalisation adjusts the data; . If your data is on very different scales (especially low to high), you would want to normalise the data. Alter each column to have compatible basic statistics. This can be helpful to make sure there is no loss of accuracy. One of the goals of model training is to identify the signal and ignore the noise if the model is given free rein to minimize error, there is a possibility of suffering from overfitting.\tRegularisation adjusts the prediction function. Regularization imposes some control on this by providing simpler fitting functions over complex ones.\n"}
{"input": "What is the cross-entropy loss function?", "output": "Cross-entropy is the commonly used loss function in deep learning for classification problems. The cross-entropy loss measures the difference between the real probability distribution and the predicted probability distribution over the classes."}
{"input": "Explain the difference between Type I and Type II errors in statistical hypothesis testing.", "output": "Type I and Type II errors are two types of incorrect conclusions in hypothesis testing. A Type I error, also known as a false positive, occurs when we reject the null hypothesis when it is true. This means we\u2019ve detected an effect or difference that doesn\u2019t actually exist. On the other hand, a Type II error, or false negative, happens when we fail to reject the null hypothesis when it\u2019s false, meaning we\u2019ve missed identifying an existing effect or difference. The probability of making a Type I error is denoted by alpha (\u03b1), while beta (\u03b2) denotes the probability of a Type II error."}
{"input": "What is stationarity in a Time Series? Why is it important?", "output": "Stationarity in a Time Series refers to the property where statistical parameters like mean, variance, and autocorrelation are constant over time. It\u2019s crucial because most time series models assume stationarity. Non-stationary data can lead to misleading statistics and unreliable forecasts. Stationarity simplifies model development as it allows usage of past behavior to predict future points."}
{"input": "What is a good metric for measuring the level of multicollinearity?", "output": "Ans. VIF or 1/tolerance is a good measure of measuring multicollinearity in models. VIF is the percentage of the variance of a predictor which remains unaffected by other predictors. So higher the VIF value, greater is the multicollinearity amongst the predictors.\n\nA rule of thumb for interpreting the variance inflation factor:\n\n1 = not correlated.\n\nBetween 1 and 5 = moderately correlated.\n\nGreater than 5 = highly correlated.\n"}
{"input": "  Explain the purpose of using a libaio package in Ubuntu?", "output": "Libaio is Linux Kernel Asynchronous I/O (A/O).  A/O allows even a single application thread to overlap I/O operations with other processing, by providing an interface for submitting one or more I/O requests in one system call without waiting for completion.  And a separate interface to reap completed I/O operations associated with a given completion group."}
{"input": "Describe Hypothesis Testing. How is the statistical significance of an insight assessed?", "output": "Hypothesis Testing in statistics is used to see if a certain experiment yields meaningful results. It essentially helps to assess the statistical significance of insight by determining the odds of the results occurring by chance. The first thing is to know the null hypothesis and then state it. Then the p-value is calculated, and if the null hypothesis is true, other values are also determined. The alpha value denotes the significance and is adjusted accordingly. If the p-value is less than alpha, the null hypothesis is rejected, but if it is greater than alpha, the null hypothesis is accepted. The rejection of the null hypothesis indicates that the results obtained are statistically significant."}
{"input": "Why partitioning and formatting is a prerequisite to installing an operating system?", "output": "Partitioning and formatting create a preparatory environment on the drive so that the operating system can be copied and installed properly. This includes allocating space on the drive, designating a drive name, determining and creating the appropriate file system and structure."}
{"input": "How do you evaluate the accuracy of a time series forecast, and what are some common metrics used for this purpose?", "output": "The accuracy of a time series forecast can be evaluated using a variety of metrics, including mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), and mean absolute percentage error (MAPE). These metrics compare the forecasted values to the actual values in the dataset, with lower values indicating better accuracy."}
{"input": "What is an Array?", "output": "The array is defined as a collection of similar items, stored in a contiguous manner. Arrays is an intuitive concept as the need to group similar objects together arises in our day to day lives. Arrays satisfy the same need. How are they stored in the memory? Arrays consume blocks of data, where each element in the array consumes one unit of memory. The size of the unit depends on the type of data being used. For example, if the data type of elements of the array is int, then 4 bytes of data will be used to store each element. For character data type, 1 byte will be used. This is implementation specific, and the above units may change from computer to computer.\n\nExample:\n\nfruits = [\u2018apple\u2019, banana\u2019, pineapple\u2019]\n\nIn the above case, fruits is a list that comprises of three fruits. To access them individually, we use their indexes. Python and C are 0- indexed languages, that is, the first index is 0. MATLAB on the contrary starts from 1, and thus is a 1-indexed language.\n"}
{"input": " Explain the framework", "output": "The framework is a platform for making software applications. It provides the basis on which developers can build programs for a specific platform. For example, a framework may include predetermined classes as well as functions. It can be used to process inputs, manage hardware, and interact with system software."}
{"input": " What is an Interface?", "output": "An interface is a way of communicating between two objects. The objects can be software or hardware. An interface can be defined in many different ways, but the most common definition is that it is a way for one object to talk to another object. This communication can take many different forms, such as sending data to the other object, receiving data from the other object, or both. An interface can be implemented in many different ways, but the most common way is to use a software class. This class defines the interface and then other classes can use this class to implement the interface. When an object uses an interface, it is saying that it understands how the interface works and can use it to communicate with other objects."}
{"input": "In committee machines, mark all the combiners that do not make direct use of the input: (i) A mixture of experts (ii) Bagging (iii) Ensemble averaging (iv) Boosting", "output": "(i), (ii), (iii). Boosting is the only one that does not make direct use of the input."}
{"input": "How would you handle an imbalanced dataset?", "output": "Sampling Techniques can help with an imbalanced dataset. There are two ways to perform sampling, Under Sample or Over Sampling.\n\nIn Under Sampling, we reduce the size of the majority class to match minority class thus help by improving performance w.r.t storage and run-time execution, but it potentially discards useful information.\n\nFor Over Sampling, we upsample the Minority class and thus solve the problem of information loss, however, we get into the trouble of having Overfitting.\n\nThere are other techniques as well \u2013\n\nCluster-Based Over Sampling \u2013 In this case, the K-means clustering algorithm is independently applied to minority and majority class instances. This is to identify clusters in the dataset. Subsequently, each cluster is oversampled such that all clusters of the same class have an equal number of instances and all classes have the same size\n\nSynthetic Minority Over-sampling Technique (SMOTE) \u2013 A subset of data is taken from the minority class as an example and then new synthetic similar instances are created which are then added to the original dataset. This technique is good for Numerical data points.\n"}
{"input": "Can you explain the concept of Granger Causality and its use in time series analysis?", "output": "Granger Causality (GC) is a statistical concept used to determine if one time series can predict another. It\u2019s not about true causality, but rather predictive capability. In GC, if a time series X \u201cGranger-causes\u201d Y, past values of X should contain information that helps predict Y beyond the information contained in past values of Y alone. In time series analysis, GC is useful for understanding relationships between different variables over time. For instance, it could be used to analyze economic data, like whether changes in GDP Granger-cause changes in unemployment rates. This would involve testing the null hypothesis that lagged values of GDP do not help in predicting current unemployment rates, against the alternative that they do. However, it\u2019s important to note that GC doesn\u2019t imply a causal relationship in a traditional sense. Just because X Granger-causes Y, it doesn\u2019t mean X truly causes Y. There might be other factors involved or the relationship could be coincidental."}
{"input": "What is correlation?", "output": "Correlation is used to test relationships between quantitative variables and categorical variables. Unlike covariance, correlation tells us how strong the relationship is between two variables. The value of correlation between two variables ranges from -1 to +1. The -1 value represents a high negative correlation, i.e., if the value in one variable increases, then the value in the other variable will drastically decrease. Similarly, +1 means a positive correlation, and here, an increase in one variable will lead to an increase in the other. Whereas, 0 means there is no correlation. If two variables are strongly correlated, then they may have a negative impact on the statistical model, and one of them must be dropped. Next up on this top Statistics Interview Questions and Answers blog, let us take a look at the intermediate set of questions."}
{"input": "What is the purpose of the assignment operator?", "output": "The assignment operator is used to store a value, string or a computational result in a variable."}
{"input": "What are the types of biases that we encounter while sampling?", "output": "Sampling biases are errors that occur when taking a small sample of data from a large population as the representation in statistical analysis. There are three types of biases: The selection bias, The survivorship bias, The undercoverage bias"}
{"input": "How do you handle missing values in a time series dataset?", "output": "There are several ways to handle missing values in a time series dataset, including interpolation, forward filling, backward filling, and imputation. The choice of method will depend on the nature and extent of the missing data, as well as the specific modeling technique being used."}
{"input": "Explain primary memory", "output": "Primary memory, RAM is the main memory of a computer which can be directly accessed by the Central Processing Unit (CPU). It stores temporary information until the process finishes its execution."}
{"input": "What is Parsing in the context of NLP?", "output": "Parsing in NLP refers to the understanding of a sentence and its grammatical structure by a machine. Parsing allows the machine to understand the meaning of a word in a sentence and the grouping of words, phrases, nouns, subjects, and objects in a sentence. Parsing helps analyze the text or the document to extract useful insights from it."}
{"input": "What is the exploding gradient problem while using the back propagation technique?", "output": "When large error gradients accumulate and result in large changes in the neural network weights during training, it is called the exploding gradient problem. The values of weights can become so large as to overflow and result in NaN values. This makes the model unstable and the learning of the model to stall just like the vanishing gradient problem. This is one of the most commonly asked interview questions on machine learning.\n"}
{"input": "What is Bayes\u2019 Theorem?", "output": "In probability theory and statistics, Bayes\u2019 Theorem refers to the probability of an event based on conditions that exist. Essentially, the theorem allows us to update our beliefs about a random event based on what we know about the event. For example, if the risk of customer churn increases the longer a user has been inactive, Bayes\u2019 Theorem allows us to more accurately assess the churn risk for users because we can condition the probability of churn to how long the user has been inactive."}
{"input": "What is the difference between discrete and continuous probability distributions?", "output": "Discrete probability distributions are used for events with countable outcomes, such as the number of customers arriving at a store. Continuous probability distributions, like the normal distribution, are used for events with an infinite number of possible outcomes, such as height or weight measurements."}
{"input": "True or False: The KL-divergence is not a distance metric.", "output": "True KLD is not a distance metric."}
{"input": "For a given token, its input representation is the sum of embedding from the token, segment and position embedding a. ELMo b. GPT c. BERT d. ULMFit", "output": "Answer: c)\n\nBERT uses token, segment and position embedding.\n"}
{"input": "What is the difference between Descriptive and Inferential Statistics?", "output": "Descriptive Statistics: Descriptive statistics is used to summarize a sample set of data like the standard deviation or the mean. Inferential statistics: Inferential statistics is used to draw conclusions from the test data that are subjected to random variations."}
{"input": "What is kurtosis?", "output": "Kurtosis is a measure of the degree of the extreme values present in one tail of distribution or the peaks of frequency distribution as compared to the others. The standard normal distribution has a kurtosis of 3 whereas the values of symmetry and kurtosis between -2 and +2 are considered normal and acceptable. The data sets with a high level of kurtosis imply that there is a presence of outliers. One needs to add data or remove outliers to overcome this problem. Data sets with low kurtosis levels have light tails and lack outliers."}
{"input": "How does dynamic host configuration protocol aid in network administration?", "output": "Instead of having to visit each client computer to configure a static IP address, the network administrator can apply dynamic host configuration protocol to create a pool of IP addresses known as scopes that can be dynamically assigned to clients."}
{"input": "What are unigrams, bigrams, trigrams, and n-grams in NLP?", "output": "When we parse a sentence one word at a time, then it is called a unigram. The sentence parsed two words at a time is a bigram. When the sentence is parsed three words at a time, then it is a trigram. Similarly, n-gram refers to the parsing of n words at a time. Therefore, parsing allows machines to understand the individual meaning of a word in a sentence. Also, this type of parsing helps predict the next word and correct spelling errors."}
{"input": "What is the use of the violin plot?", "output": "The name violin plot has been derived from the shape of the graph which matches the violin. This graph is an extension of the Kernel Density Plot along with the properties of the boxplot. All the statistical measures shown by a boxplot are also shown by the violin plot but along with this, The width of the violin represents the density of the variable in the different regions of values. This visualization tool is generally used in the exploratory data analysis step to check the distribution of the continuous data variables. With this, we have covered some of the most important Machine Learning concepts which are generally asked by the interviewers to test the technical understanding of a candidate also, we would like to wish you all the best for your next interview."}
{"input": "What are the main assumptions made in a Bayesian statistical model?", "output": "The main assumptions made in a Bayesian statistical model are that the data is generated by a process that is governed by certain underlying parameters, and that these parameters have a certain probability distribution. The model also assumes that the data is observed, and that the parameters are unknown."}
{"input": "The rectified linear unit, or ReLU g(z) = max{0, z} is the default for many CNN architectures. In what sense is the ReLU better than traditional sigmoidal activation functions?", "output": "The ReLU function has the benefit of not saturating for positive inputs since its derivative is one for any positive value."}
{"input": "What are the scenarios where outliers are kept in the data?", "output": "There are not many scenarios where outliers are kept in the data, but there are some important situations when they are kept. They are kept in the data for analysis if: Results are critical. Outliers add meaning to the data. The data is highly skewed"}
{"input": "Can you explain what MCMC sampling is?", "output": "MCMC sampling is a method for approximating a target distribution by sampling from a Markov chain whose stationary distribution is the target distribution. This can be done by running the Markov chain for a large number of steps and then taking a sample from the final state of the chain."}
{"input": "Is logistic regression a generative or a descriptive classifier? Why?", "output": "Logistic regression is a descriptive model. Logistic regression learns to classify by knowing what features differentiate two or more classes of objects. For example, to classify between an apple and an orange, it will learn that the orange is orange in color and an apple is not. On the other hand, a generative classifier like a Naive Bayes will store all the classes' critical features and then classify based on the features the test case best fits."}
{"input": "What is the Central Limit Theorem and why is it important in the field of mathematical statistics?", "output": "The Central Limit Theorem (CLT) is a fundamental concept in statistics, stating that when an adequately large number of independent, identically distributed random variables are added together, their sum tends towards a normal distribution regardless of the original sample\u2019s shape. This theorem underpins many statistical methods and concepts, including confidence intervals and hypothesis testing. Importantly, CLT allows for simplification in problem-solving as it enables us to make assumptions about the population from which we\u2019re sampling. It also provides a basis for inferential statistics, allowing predictions about populations based on sample data. Furthermore, it justifies using the mean of a large sample as an approximation of the population mean, reducing complexity in calculations. In essence, the importance of CLT lies in its ability to simplify complex statistical problems by providing a common ground \u2013 the normal distribution \u2013 irrespective of the initial distribution type."}
{"input": "How would you diagnose the residuals of a time series model?", "output": "To diagnose the residuals of a time series model, I would use graphical analysis and statistical tests. Graphical methods include plotting the residuals against time to check for patterns or trends that suggest non-stationarity. A correlogram or autocorrelation function (ACF) plot can be used to detect autocorrelation in the residuals. If there are significant spikes at lagged intervals, it indicates autocorrelation. Statistical tests such as Ljung-Box test can also be employed. This test checks if a group of autocorrelations of a time series is different from zero. Another useful tool is the Durbin-Watson statistic which detects first-order serial correlation.In addition, normality tests like Shapiro-Wilk or Kolmogorov-Smirnov can be applied on residuals to ensure they follow a normal distribution. Non-normal distribution may indicate presence of outliers or skewness in the data."}
{"input": "A data set is given to you and it has missing values which spread along 1 standard deviation from the mean. How much of the data would remain untouched?", "output": "It is given that the data is spread across mean that is the data is spread across an average. So, we can presume that it is a normal distribution. In a normal distribution, about 68% of data lies in 1 standard deviation from averages like mean, mode or median. That means about 32% of the data remains uninfluenced by missing values.\n"}
{"input": " What are the Techniques Used to Avoid Overfitting?", "output": "Cross-validation: This is a technique where the data is split into multiple subsets, and the model is trained and tested on different subsets. This helps to prevent the model from memorizing the training data and generalizing poorly to new data.Regularization: This is a technique where a penalty term is added to the model's objective function, which discourages the model from assigning too much importance to any single feature. This helps to prevent the model from fitting to noise in the training data.Early stopping: This is a technique where the training process is stopped before the model's performance on the training data starts to decrease, this is useful when the model is trained with multiple iterations.Ensemble methods: This is a technique where multiple models are trained, and their predictions are combined to create a final prediction. This helps to reduce the variance and increase the robustness of the model.Pruning: This is a technique where the complexity of the model is reduced by removing unimportant features or nodes.Dropout: This is a technique where a random subset of the neurons is dropped out of the network during training, this prevents the network from relying too much on any one neuron.Bayesian approaches: This is a technique where prior information is incorporated into the model's parameters."}
{"input": "What is the significance of Gamma and Regularization in SVM?", "output": "The gamma defines influence. Low values meaning \u2018far\u2019 and high values meaning \u2018close\u2019.  If gamma is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with C will be able to prevent overfitting.  If gamma is very small, the model is too constrained and cannot capture the complexity of the data.\n\nThe regularization parameter (lambda) serves as a degree of importance that is given to miss-classifications. This can be used to draw the tradeoff with OverFitting.\n"}
{"input": "How do you check for data normality in Descriptive Statistics?", "output": "Checking for data normality is essential for determining whether parametric or non-parametric statistical methods are appropriate for analysis. Common methods to assess normality include visual inspection using histograms or Q-Q plots, and statistical tests like the Shapiro-Wilk test or the Kolmogorov-Smirnov test."}
{"input": "  What is the proper termination rate for UTP cables?", "output": "The proper termination for unshielded twisted pair network cable is 100 ohms."}
{"input": "What is Multilayer Perceptron? and How it is different from a single-layer perceptron?", "output": "A multilayer perceptron (MLP) is an advancement of the single-layer perceptron which uses more than one hidden layer to process the data from input to the final prediction. It consists of multiple layers of interconnected neurons, with multiple nodes present in each layer. The MLP architecture is referred to as the feedforward neural network because data flows in one direction, from the input layer through one or more hidden layers to the output layer. The differences between the single-layer perceptron and multilayer perceptron are as follows: \n Architecture: A single-layer perceptron has only one layer of neurons, which takes the input and produces an output. While a multilayer perceptron has one or more hidden layers of neurons between the input and output layers. \n Complexity: A single-layer perceptron is a simple linear classifier that can only learn linearly separable patterns. While a multilayer perceptron can learn more complex and nonlinear patterns by using nonlinear activation functions in the hidden layers. \n Learning: Single-layer perceptrons use a simple perceptron learning rule to update their weights during training. While multilayer perceptrons use a more complex backpropagation algorithm to train their weights, which involves both forward propagations of input through the network and backpropagation of errors to update the weights. \n Output: Single-layer perceptrons produce a binary output, indicating which of two possible classes the input belongs to. Multilayer perceptrons can produce real-valued outputs, allowing them to perform regression tasks in addition to classification. \n Applications: Single-layer perceptrons are suitable for simple linear classification tasks whereas Multilayer perceptrons are more suitable for complex classification tasks where the input data is not linearly separable, as well as for regression tasks where the output is continuous variables."}
{"input": "What is the impact of outliers in statistics?", "output": "Outliers in statistics have a very negative impact as they skew the result of any statistical query. For example, if we want to calculate the mean of a dataset that contains outliers, then the mean calculated will be different from the actual mean (i.e., the mean we will get once we remove the outliers)."}
{"input": "How does linear regression work?", "output": "When utilised in statistics, linear regression is a technique that models the relationship between one or more predictor variables and one outcome variable. For example, linear regression may be used to study the connection between various predictors, such as age, gender, heredity, diet, and height."}
{"input": "What is the meaning of an inlier?", "output": "An Inliner is a data point within a data set that lies at the same level as the others. It is usually an error and is removed to improve the model accuracy. Unlike outliers, inlier is hard to find and often requires external data for accurate identification. "}
{"input": "Claud Shannon's paper \u201cA mathematical theory of communication\u201d, marked the birth of information theory. Published in 1948, it has become since the Magna Carta of the information age. Describe in your own words what is meant by the term Shannon bit.", "output": "The Shannon bit has two distinctive states; it is either 0 or 1, but never both at the same time. Shannon devised an experiment in which there is a question whose only two possible answers were equally likely to happen. He then defined one bit as the amount of information gained (or alternatively, the amount of entropy removed) once an answer to the question has been learned. He then continued to state that when the a-priori probability of any one possible answer is higher than the other, the answer would have conveyed less than one bit of information. "}
{"input": "What Are the Three Stages of Building a Model in Machine Learning?", "output": "To build a model in machine learning, you need to follow few steps:\n\nUnderstand the business model\n\nData acquisitions\n\nData cleaning\n\nExploratory data analysis\n\nUse machine learning algorithms to make a model\n\nUse unknown dataset to check the accuracy of the model\n"}
{"input": "When does regularization come into play in Machine Learning?", "output": "At times when the model begins to underfit or overfit, regularization becomes necessary. It is a regression that diverts or regularizes the coefficient estimates towards zero. It reduces flexibility and discourages learning in a model to avoid the risk of overfitting. The model complexity is reduced and it becomes better at predicting.\n"}
{"input": "What is an operating system and what is its importance in business?", "output": "An operating system is a software program that allows hardware and other software to perform their regular functions. Operating systems control access to internal and external storage, input and output devices and network connectivity. The main purpose of an operating system is to allocate the memory necessary for running software, which allows organizations to use computers for everyday functions. Enterprise businesses require more complex operating systems, with features like server operating systems, to meet the rigorous IT demands of the organization."}
{"input": " Why do we take smaller values of the learning rate?", "output": "Smaller values of learning rate help the training process to converge more slowly and gradually toward the global optimum instead of fluctuating around it. This is because a smaller learning rate results in smaller updates to the model weights at each iteration, which can help to ensure that the updates are more precise and stable.If the learning rate is too large, the model weights can update too quickly, which can cause the training process to overshoot the global optimum and miss it entirely."}
{"input": "If you are using C language to implement the heterogeneous linked list, what pointer type should be used?", "output": "The heterogeneous linked list contains different data types, so it is not possible to use ordinary pointers for this. For this purpose, you have to use a generic pointer type like void pointer because the void pointer is capable of storing a pointer to any type."}
{"input": "How do you select important variables while working on a data set?", "output": "There are various means to select important variables from a data set that include the following:\n\nIdentify and discard correlated variables before finalizing on important variables\n\nThe variables could be selected based on \u2018p\u2019 values from Linear Regression\n\nForward, Backward, and Stepwise selection\n\nLasso Regression\n\nRandom Forest and plot variable chart\n\nTop features can be selected based on information gain for the available set of features.\n"}
{"input": "What is stepwise selection method?", "output": "\u00b7 Forward \u2014 Starts with zero variables. If a variable is added then it stays in the model even if it becomes insignificant later. \u00b7 Backward \u2014 Starts with all the variables. If a variable is eliminated then it cannot be included in the model. \u00b7 Stepwise \u2014 Includes aspects of forward and backward selection methods. It terminates when no variable can be added or removed from the model."}
{"input": "What are Intelligent Agents, and How are They Used in AI?", "output": "Intelligent agents are autonomous entities that use sensors to know what is going on, and then use actuators to perform their tasks or goals. They can be simple or complex and can be programmed to learn to accomplish their jobs better."}
{"input": "Which algorithms can be used for important variable selection?", "output": "Ans. Random Forest, Xgboost and plot variable importance charts can be used for variable selection.\n"}
{"input": " What is the use of a default gateway?", "output": "Default gateways provide means for the local networks to connect to the external network. The default gateway for connecting to the external network is usually the address of the external router port."}
{"input": "Explain the handling of missing or corrupted values in the given dataset.", "output": "An easy way to handle missing values or corrupted values is to drop the corresponding rows or columns. If there are too many rows or columns to drop then we consider replacing the missing or corrupted values with some new value.\n\nIdentifying missing values and dropping the rows or columns can be done by using IsNull() and dropna( ) functions in Pandas. Also, the Fillna() function in Pandas replaces the incorrect values with the placeholder value.\n"}
{"input": " Why we cannot use linear regression for a classification task?", "output": "The main reason why we cannot use linear regression for a classification task is that the output of linear regression is continuous and unbounded, while classification requires discrete and bounded output values. If we use linear regression for the classification task the error function graph will not be convex. A convex graph has only one minimum which is also known as the global minima but in the case of the non-convex graph, there are chances of our model getting stuck at some local minima which may not be the global minima. To avoid this situation of getting stuck at the local minima we do not use the linear regression algorithm for a classification task."}
{"input": "What are some of the limitations of Deep Learning?", "output": "There are a few disadvantages of Deep Learning as mentioned below: Networks in Deep Learning require a huge amount of data to train well. Deep Learning concepts can be complex to implement sometimes. Achieving a high amount of model efficiency is difficult in many cases."}
{"input": " What is an operating system?", "output": "An operating system is a collection of software that runs on a computer and provides the basic functions of storing and managing information, such as memory, storage devices, and network access. The operating system controls how the computer works and how it interacts with other software and hardware. For example, an operating system can control the way a computer stores and accesses data, such as files and printers. In addition to controlling how the computer works and operating system can also provide many other functions, such as security, communication, and user interface. An operating system can also be used to develop new software applications."}
{"input": "TF-IDF helps you to establish? a. most frequently occurring word in document b. the most important word in the document", "output": "Answer: b)\n\nTF-IDF helps to establish how important a particular word is in the context of the document corpus. TF-IDF takes into account the number of times the word appears in the document and is offset by the number of documents that appear in the corpus.\n\nTF is the frequency of terms divided by the total number of terms in the document.\n\nIDF is obtained by dividing the total number of documents by the number of documents containing the term and then taking the logarithm of that quotient.\n\nTf.idf is then the multiplication of two values TF and IDF.\n\nSuppose that we have term count tables of a corpus consisting of only two documents, as listed here:\n\nTerm\tDocument 1 Frequency\tDocument 2 Frequency\n\nThis\t1\t1\n\nis\t1\t1\n\na\t2\t \n\nSample\t1\t \n\nanother \t \t2\n\nexample\t \t3\n\nThe calculation of tf\u2013idf for the term \u201cthis\u201d is performed as follows:\n\nfor 'this'\n\n-----------\n\ntf('this', d1) = 1/5 = 0.2\n\ntf('this', d2) = 1/7 = 0.14\n\nidf('this', D) = log (2/2) =0\n\nhence tf-idf\n\ntfidf('this', d1, D) = 0.2* 0 = 0\n\ntfidf('this', d2, D) = 0.14* 0 = 0\n\nfor 'example'\n\n------------\n\ntf('example', d1) = 0/5 = 0\n\ntf('example', d2) = 3/7 = 0.43\n\nidf('example', D) = log(2/1) = 0.301\n\ntfidf('example', d1, D) = tf('example', d1) * idf('example', D) = 0 * 0.301 = 0\n\ntfidf('example', d2, D) = tf('example', d2) * idf('example', D) = 0.43 * 0.301 = 0.129\n\nIn its raw frequency form, TF is just the frequency of the \u201cthis\u201d for each document. In each document, the word \u201cthis\u201d appears once; but as document 2 has more words, its relative frequency is smaller.\n\nAn IDF is constant per corpus, and accounts for the ratio of documents that include the word \u201cthis\u201d. In this case, we have a corpus of two documents and all of them include the word \u201cthis\u201d. So TF\u2013IDF is zero for the word \u201cthis\u201d, which implies that the word is not very informative as it appears in all documents.\n\nThe word \u201cexample\u201d is more interesting \u2013 it occurs three times, but only in the second document. To understand more about NLP, check out these NLP projects.\n"}
{"input": "What is shattering a set of points? Explain VC dimension.", "output": "In order to shatter a given configuration of points, a classifier must be able to, for all possible assignments of positive and negative for the points, perfectly partition the plane such that positive points are separated from negative points. For a configuration of n points, there are 2n possible assignments of positive or negative. \n\nWhen choosing a classifier, we need to consider the type of data to be classified and this can be known by VC dimension of a classifier. It is defined as cardinality of the largest set of points that the classification algorithm i.e. the classifier can shatter. In order to have a VC dimension of at least n, a classifier must be able to shatter a single given configuration of n points.\n"}
{"input": "Symmetric distributions need to be unimodal, does it?", "output": "Bi- or multi-modal symmetric distributions do not have to have only one mode or value that occurs most frequently, nor do they have to be unimodal (having only one mode or value that occurs most frequently)."}
{"input": "Can linear regression be used for representing quadratic equations?", "output": "Yes, paradoxically, a multiple linear regression model can be used to represent quadratic equations. For more complex linear regression models, we use multiple independent variables to predict the dependent variable. Such a linear regression model is called a multiple linear regression model. A linear model with multiple dependent variables x1, x2, ..., xn can be written as y=1x1+2x2+...+nxn. For a quadratic function given by, say, y=ax2+bx+c, we can use x1=x2, x2=x, and x3=1, effectively representing the desired quadratic equation. Similarly, linear regression models can be used to describe higher-order polynomials as well."}
{"input": "Complete the missing parts regarding the VGG19 CNN architecture: 1. The VGG19 CNN consists of [...] layers. 2. It consists of [...] convolutional and 3 [...] layers. 3. The input image size is [...]. 4. The number of input channels is [...]. 5. Every image has it\u2019s mean RGB value [subtracted / added]. 6. Each convolutional layer has a [small/large] kernel sized [...]. 7. The number of pixels for padding and stride is [...]. 8. There are 5 [...] layers having a kernel size of [...] and a stride of [...] pixels. 9. For non-linearity a [rectified linear unit (ReLU [5])/sigmoid] is used. 10. The [...] FC layers are part of the linear classifier. 11. The first two FC layers consist of [...] features. 12. The last FC layer has only [...] features. 13. The last FC layer is terminated by a [...] activation layer. 14. Dropout [is / is not] being used between the FC layers.", "output": "1. The VGG19 CNN consists of 19 layers. 2. It consists of 5 convolutional and 3 FC layers. 3. The input image size is 244 , the default size most ImageNet trained CNNs work on. 4. The number of input channels is 3 . 5. Every image has its mean RGB value subtracted . (why?) 6. Each convolutional layer has a small kernel sized 3 * 3 . (why?) 7. The number of pixels for padding and stride is the same and equals 1 . 8. There are 5 convolutional layers having a kernel size of 2 * 2 and a stride of 2 pixels. 9. For non-linearity a rectified linear unit (ReLU [5]) is used. 10. The 3 FC layers are part of the linear classifier. 11. The first two FC layers consist of 4096 features. 12. The last FC layer has only 1000 features. 13. The last FC layer is terminated by a softmax activation layer. 14. Dropout is being used between the FC layers."}
{"input": "True or False: The cross-correlation operator is used to find the location where two different signals are most similar.", "output": "True."}
{"input": "What is NIC?", "output": "NIC is short for Network Interface Card. This is a peripheral card that is attached to a PC in order to connect to a network. Every NIC has its own MAC address that identifies the PC on the network."}
{"input": "What is kernel?", "output": "A kernel is the core of every operating system. It connects applications to the actual processing of data. It also manages all communications between software and hardware components to ensure usability and reliability."}
{"input": "What is a voting model?", "output": "Ans. A voting model is an ensemble model which combines several classifiers but to produce the final result, in case of a classification-based model, takes into account, the classification of a certain data point of all the models and picks the most vouched/voted/generated option from all the given classes in the target column.\n"}
{"input": "What do you understand by Natural Language Processing?", "output": "Natural Language Processing is a field of computer science that deals with communication between computer systems and humans. It is a technique used in Artificial Intelligence and Machine Learning. It is used to create automated software that helps understand human-spoken languages to extract useful information from the data. Techniques in NLP allow computer systems to process and interpret data in the form of natural languages."}
{"input": "How does the term covariance relate to understanding?", "output": "When two items are associated in a random process, covariance is the measure of how closely they fluctuate together. Is there a connection between one of the variables in a random pair and the other variable? If there is, then the systematic connection is determined."}
{"input": "How is missing data handled in statistics?", "output": "There are many ways to handle missing data in Statistics: Prediction of the missing values, Assignment of individual (unique) values, Deletion of rows, which have the missing data, Mean imputation or median imputation, Using random forests, which support the missing values"}
{"input": " What is a snooping protocol", "output": "A snooping protocol, also referred to as a 'bus-snooping protocol,' maintains cache coherency in symmetric multiprocessing environments. All caches on the bus snoop or monitor the bus to determine if they have a copy of the block of data that is requested on the bus. Each cache holds a copy of the sharing status of every block of physical memory it has. Typically, several copies of a file in a multiprocessing environment can be read without any problem of coherence. However, a processor should have exclusive access to the bus to write."}
{"input": "Explain how Automatic Differentiation uses floating point numerical rather than symbolic expressions.", "output": "The procedure of Automatic Differentiation is to use verbatim text of a computer program which calculates a numerical value and to transform it into the text of a computer program called the transformed program which calculates the desired derivative values. The transformed computer program carries out these derivative calculations by repeated use of the chain rule however applied to actual floating point values rather than to a symbolic representation."}
{"input": "How do you validate a time series model?", "output": "To validate a time series model, you first split the data into training and testing sets. The model is then fitted on the training set and predictions are made for the testing set. These predictions are compared with actual values to assess accuracy using metrics such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), or Mean Absolute Percentage Error (MAPE). Next, residual diagnostics are performed by analyzing the residuals\u2019 pattern. Ideally, they should be white noise, meaning they\u2019re normally distributed around zero without any autocorrelation. If not, it indicates model inadequacy. Lastly, forecast validation is done by checking if the model can predict future observations accurately. This involves generating forecasts for a period beyond the dataset and comparing them with actuals when available."}
{"input": "How do you formulate null and alternative hypotheses in hypothesis testing?", "output": "The null hypothesis (H0) is the default assumption that there is no significant difference or relationship between variables. The alternative hypothesis (Ha) represents the assertion we want to test, suggesting that there is a significant difference or relationship. For example, in testing a new drug\u2019s effectiveness, the null hypothesis could be \u201cThe new drug has no effect,\u201d while the alternative hypothesis could be \u201cThe new drug is effective in reducing symptoms.\u201d"}
{"input": " How do you generate random numbers in C?", "output": "Random numbers are generated in C using the rand() command. For example: anyNum = rand() will generate any integer number beginning from 0, assuming that anyNum is a variable of type integer."}
{"input": "What is the meaning of bagging and boosting in Deep Learning?", "output": "Bagging is the concept of splitting a dataset and randomly placing it into bags for training the model. Boosting is the scenario where incorrect data points are used to force the model to produce the wrong output. This is used to retrain the model and increase accuracy."}
{"input": " What's the relationship between computers and operating systems?", "output": "A multiprogramming OS allows a process wherein multiple programs execute over one central processing unit (CPU). If your interviewer asks about multiprogramming, highlight the key differences between a multiprogramming OS and other systems. Show that you understand the benefits of multiprogramming by providing a real-life example that resulted in positive outcomes, such as shorter response times."}
{"input": "What would be the hazards of letting users sneak a peek at the other bucket in an A/B test?", "output": "The user might not act the same suppose had they not seen the other bucket. You are essentially adding additional variables of whether the user peeked the other bucket, which are not random across groups."}
{"input": "Explain the working principle of SVM.", "output": "A data set that is not separable in different classes in one plane may be separable in another plane. This is exactly the idea behind the SVM in this a low dimensional data is mapped to high dimensional data so, that it becomes separable in the different classes. A hyperplane is determined after mapping the data into a higher dimension which can separate the data into categories. SVM model can even learn non-linear boundaries with the objective that there should be as much margin as possible between the categories in which the data has been categorized. To perform this mapping different types of kernels are used like radial basis kernel, gaussian kernel, polynomial kernel, and many others."}
{"input": "Can you describe the use of Bayesian Inference in machine learning algorithms?", "output": "Bayesian Inference is a statistical method in machine learning that updates the probability for a hypothesis as more evidence becomes available. It\u2019s used to predict future data points by incorporating prior knowledge and observed data. Bayesian inference algorithms, such as Naive Bayes classifiers, are popular due to their simplicity, efficiency, and effectiveness in high-dimensional spaces. They\u2019re often used in text classification, spam filtering, and recommendation systems. The Bayesian approach also provides a principled framework for dealing with uncertainty and making decisions under uncertainty, which is crucial in many machine learning applications."}
{"input": "What is the meaning of model capacity in Deep Learning?", "output": "In Deep Learning, model capacity refers to the capacity of the model to take in a variety of mapping functions. Higher model capacity means a large amount of information can be stored in the network. We will check out neural network interview questions alongside as it is also a vital part of Deep Learning."}
{"input": "What is a MAC Address?", "output": "MAC address is a unique identifier that is assigned to a NIC (Network Interface Controller/ Card). It consists of a 48 bit or 64-bit address, which is associated with the network adapter. MAC address can be in hexadecimal format. The full form of MAC address is Media Access Control address."}
{"input": "True or false: When using a single model, the risk of overfitting the data increases when the number of adjustable parameters is large compared to cardinality (i.e., size of the set) of the training corpus.", "output": "True"}
{"input": "How do you choose the appropriate significance level (alpha) for hypothesis testing?", "output": "The significance level (alpha) is the probability of making a Type I error. It is typically set to 0.05 (5%) in most cases, indicating a 5% chance of rejecting the null hypothesis when it is true. The choice of alpha depends on the consequences of making Type I errors and the level of certainty required for the analysis."}
{"input": "What is the process of carrying out a linear regression?", "output": "Linear Regression Analysis consists of more than just fitting a linear line through a cloud of data points. It consists of 3 stages\u2013\n\n analyzing the correlation and directionality of the data,\n\n estimating the model, i.e., fitting the line, \n\nevaluating the validity and usefulness of the model.\n"}
{"input": "What are quantitative data and qualitative data?", "output": "Qualitative data is used to describe the characteristics of data and is also known as Categorical data. For example, how many types. Quantitative data is a measure of numerical values or counts. For example, how much or how often. It is also known as Numeric data."}
{"input": "Explain how we can do parsing.", "output": "Parsing is the method to identify and understand the syntactic structure of a text. It is done by analyzing the individual elements of the text. The machine parses the text one word at a time, then two at a time, further three, and so on. When the machine parses the text one word at a time, then it is a unigram. When the text is parsed two words at a time, it is a bigram. The set of words is a trigram when the machine parses three words at a time."}
{"input": "Describe the process of decomposing a time series into its components.", "output": "Time series decomposition involves breaking down a time series into its constituent elements: trend, seasonality, and residuals. The trend component represents the overall pattern of the data over time, often smoothed to highlight long-term changes. Seasonality captures regular patterns due to seasonal factors, typically observed within fixed periods (e.g., daily, monthly). Residuals are what remains after removing trend and seasonality, representing irregular fluctuations or \u2018noise\u2019. Decomposition can be additive or multiplicative. In an additive model, we assume that components add up to form the original series. For a multiplicative model, they multiply together. Choice depends on whether variations in data increase proportionally with the level of the series (multiplicative) or remain constant (additive). The process begins by identifying and estimating the trend using techniques like moving averages. Next, we estimate seasonality by averaging, for each time unit, over all periods. We then remove both estimated components from the original series to get residuals."}
{"input": "How would you check if a given time series is stationary?", "output": "To check if a time series is stationary, we can use both visual methods and statistical tests. Visually, plot the data to see if there\u2019s any obvious trend or seasonality. Also, observe the mean and variance over different intervals; they should remain constant for stationarity. Statistical tests like the Augmented Dickey-Fuller (ADF) test can be used. The null hypothesis of ADF is that the time series possesses a unit root and is non-stationary. If the p-value obtained from the test is less than the significance level (0.05), we reject the null hypothesis and infer the time series is stationary. Another method is the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test where the null hypothesis is reversed i.e., the time series is stationary. If the p-value is greater than 0.05, we fail to reject the null hypothesis, indicating stationarity."}
{"input": "Define Bayes theorem in terms of prior, evidence and likelihood.", "output": "Probability of event A given that event B is true (Posterior) can be found using the expression: Posterior = Likelihood * Prior / Evidence"}
{"input": "Give examples of machine learning models with high bias", "output": "Linear Regression, Logistic Regression"}
{"input": "What are the elements in TensorFlow that are programmable?", "output": "In TensorFlow, users can program three elements: Constants, Variables, Placeholders"}
{"input": "What are global variables and how do you declare them?", "output": "Global variables are variables that can be accessed and manipulated anywhere in the program. To make a variable global, place the variable declaration on the upper portion of the program, just after the preprocessor directives section."}
{"input": " What is the maximum segment length of a 100Base-FX network?", "output": "The maximum length for a network segment using 100Base-FX is 412 meters."}
{"input": "How to deal with very few data samples? Is it possible to make a model out of it?", "output": "Ans. If very few data samples are there, we can make use of oversampling to produce new data points. In this way, we can have new data points.\n"}
{"input": "How do you define Bessel's correction?", "output": "Bessel\u2019s correction corrects the flaw in using a sample to estimate a population standard deviation. It lowers the bias in the estimated standard deviation, resulting in more accurate measurements."}
{"input": "What is the difference between deep learning, machine learning and AI?", "output": "Both Deep Learning and Machine Learning are part of Artificial intelligence and the difference between all the three domains is really just about the specificities. While deep learning deals with neural networks attempting to train machines through several layers of logic, machine learning is all about algorithms which uses historical data to teach machines. Artificial intelligence, of course, is the broader term which refers to any method which helps machines to mimic basic human actions.\n"}
{"input": " What is an algorithm?", "output": "An algorithm is a finite set of steps which, if followed, accomplish a particular task. An algorithm must be clear, finite and effective."}
{"input": "The standard normal curve has a total area to be under one, and it is symmetric around zero. True or False?", "output": "True, a normal curve will have the area under unity and the symmetry around zero in any distribution. Here, all of the measures of central tendencies are equal to zero due to the symmetric nature of the standard normal curve."}
{"input": "What is normal distribution?", "output": "Ans. The distribution having the below properties is called normal distribution. \n\nThe mean, mode and median are all equal.\n\nThe curve is symmetric at the center (i.e. around the mean, \u03bc).\n\nExactly half of the values are to the left of center and exactly half the values are to the right.\n\nThe total area under the curve is 1.\n"}
{"input": "When a syntax error occurs?", "output": "A syntax error occurs when the program violates one or more grammatical rules of the programming language. These errors are detected at compile time, i.e., when the translator (compiler or interpreter) attempts to translate the program."}
{"input": "Why can't we use the mean square error cost function used in linear regression for logistic regression?", "output": "If we use mean square error in logistic regression, the resultant cost function will be non-convex, i.e., a function with many local minima, owing to the presence of the sigmoid function in h(x). As a result, an attempt to find the parameters using gradient descent may fail to optimize cost function properly. It may end up choosing a local minima instead of the actual global minima."}
